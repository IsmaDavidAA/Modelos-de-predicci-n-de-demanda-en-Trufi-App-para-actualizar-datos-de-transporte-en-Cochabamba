{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "989b05c9",
   "metadata": {},
   "source": [
    "# Importar bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b291dfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from shapely.geometry import shape, Point\n",
    "import geopandas as gpd\n",
    "from geopy.distance import distance\n",
    "import pandas as pd\n",
    "from h3 import h3\n",
    "from geopy.distance import geodesic\n",
    "import shapefile\n",
    "from shapely.geometry import Point, shape\n",
    "from shapely.ops import unary_union\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import col, lit, concat_ws, collect_set\n",
    "from pyspark.sql.types import TimestampType, StructField, StringType, IntegerType, StructType\n",
    "from pyspark.sql.functions import date_format\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.sql import functions as F\n",
    "import json\n",
    "import folium\n",
    "import io\n",
    "import ast  # Para convertir la cadena de coordenadas a una tupla\n",
    "from IPython.display import IFrame\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, dayofweek\n",
    "from pyspark.sql.functions import when\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f468c765",
   "metadata": {},
   "source": [
    "# Obtener rutas de los Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf71b5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener la ruta de la carpeta del notebook\n",
    "notebook_folder = os.getcwd()\n",
    "root_project = os.path.abspath(os.path.join(notebook_folder, '..'))\n",
    "dataset_logs = os.path.abspath(os.path.join(root_project, 'Datos', 'Logs'))\n",
    "trufi_datos = os.path.abspath(os.path.join(root_project, 'Datos', 'Registros de Trufi App'))\n",
    "municipios_datos = os.path.abspath(os.path.join(root_project, 'Datos', 'Poligonos de Cochabamba','region_cochabamba_2018.geojson'))\n",
    "clima_datos = os.path.abspath(os.path.join(root_project, 'Datos', 'Datos del clima','weather.csv'))\n",
    "lagos_datos = os.path.abspath(os.path.join(root_project, 'Datos', 'Poligonos de Cochabamba','lagos.shx'))\n",
    "h3_datos = os.path.abspath(os.path.join(root_project, 'Datos', 'Registros de Trufi App','id_index_h3.csv'))\n",
    "map_cochabamba_file_path =  os.path.abspath(os.path.join(root_project, 'Datos', 'mapas','h3_map_cochabamba_.html'))\n",
    "temporal_serie_data = os.path.abspath(os.path.join(root_project, 'Datos', 'Registros de Trufi App','temporal_serie_data'))\n",
    "csv_file_path = os.path.join(trufi_datos, 'origen-destino.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192de2a9",
   "metadata": {},
   "source": [
    "# Extraer solicitudes de logs a origen-destino.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed0542a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las rutas que cumplen con el patrón se han guardado en D:\\Trufiapp\\GANS\\Anexos\\Datos\\Registros de Trufi App\\r_origen-destino.csv.\n"
     ]
    }
   ],
   "source": [
    "# Definir la zona horaria de La Paz, Bolivia\n",
    "la_paz_timezone = pytz.timezone('America/La_Paz')\n",
    "def extract_route_info(log_line):\n",
    "    # Expresión regular para extraer información específica de las solicitudes de rutas\n",
    "    route_pattern_with_id = re.compile(r'GET /otp/plan\\?fromPlace=([-0-9.]+)%2C([-0-9.]+)&toPlace=([-0-9.]+)%2C([-0-9.]+).*?Trufi/.*?/([a-f0-9-]+)')\n",
    "    route_pattern_without_id = re.compile(r'GET /otp/plan\\?fromPlace=([-0-9.]+)%2C([-0-9.]+)&toPlace=([-0-9.]+)%2C([-0-9.]+)')\n",
    "\n",
    "    match_with_id = route_pattern_with_id.search(log_line)\n",
    "    match_without_id = route_pattern_without_id.search(log_line)\n",
    "\n",
    "    # Inicializar variables con valores predeterminados\n",
    "    origin_latitude = origin_longitude = dest_latitude = dest_longitude = id_user = None\n",
    "\n",
    "    try:\n",
    "        if match_with_id:\n",
    "            origin_latitude, origin_longitude, dest_latitude, dest_longitude, id_user = match_with_id.groups()\n",
    "        elif match_without_id:\n",
    "            origin_latitude, origin_longitude, dest_latitude, dest_longitude = match_without_id.groups()\n",
    "            id_user = 'N/A'  # Asignar un valor predeterminado\n",
    "        return origin_latitude, origin_longitude, dest_latitude, dest_longitude, id_user\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error al procesar la línea: {log_line}\")\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    return None\n",
    "def process_log_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'GET /otp/plan' in line:\n",
    "                date_str = re.search(r'\\[([^:]+:[^ ]+)', line).group(1)\n",
    "                \n",
    "                # Convertir la fecha a un objeto datetime y añadir la zona horaria de La Paz\n",
    "                date_time = datetime.strptime(date_str, '%d/%b/%Y:%H:%M:%S').replace(tzinfo=pytz.utc).astimezone(la_paz_timezone)\n",
    "\n",
    "                route_info = extract_route_info(line)\n",
    "                if route_info:\n",
    "                    origin_latitude, origin_longitude, dest_latitude, dest_longitude, id_user = route_info\n",
    "                    \n",
    "                    yield [date_time.strftime('%Y-%m-%d %H:%M:%S'),origin_latitude, origin_longitude, dest_latitude, dest_longitude, id_user]\n",
    "                    \n",
    "# Obtener la lista de archivos y ordenarlos según el prefijo numérico\n",
    "file_pattern = re.compile(r'^\\d{2}-')\n",
    "files = os.listdir(dataset_logs)\n",
    "log_files = [file for file in files if file_pattern.match(file)]\n",
    "log_files.sort(key=lambda x: int(x.split('-')[0]))\n",
    "\n",
    "header = ['date','origin_latitude', 'origin_longitude', 'destination_latitude', 'destination_longitude','userID']\n",
    "\n",
    "with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(header)\n",
    "\n",
    "    for log_file in log_files:\n",
    "        file_path = os.path.join(dataset_logs, log_file)\n",
    "        \n",
    "        route_info_generator = process_log_file(file_path)\n",
    "        \n",
    "        # Escribir las líneas en el archivo CSV\n",
    "        csv_writer.writerows(route_info_generator)\n",
    "\n",
    "print(f\"Las rutas que cumplen con el patrón se han guardado en {csv_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a281fa43",
   "metadata": {},
   "source": [
    "## Descartar solicitudes sin user ID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "843ba744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se eliminaron 83 filas sin userID de 1577937.\n"
     ]
    }
   ],
   "source": [
    "def filter_requests_with_userid(csv_file_path):\n",
    "    # Leer el archivo CSV\n",
    "    df = pd.read_csv(csv_file_path, parse_dates=['date'])\n",
    "\n",
    "    # Contar filas antes de la filtración\n",
    "    total_rows_before = len(df)\n",
    "\n",
    "    # Filtrar las solicitudes que tienen userID\n",
    "    df_filtered = df[df['userID'].notnull()]\n",
    "\n",
    "    # Contar filas después de la filtración\n",
    "    total_rows_after = len(df_filtered)\n",
    "\n",
    "    # Calcular cuántas filas fueron eliminadas\n",
    "    rows_removed = total_rows_before - total_rows_after\n",
    "\n",
    "    # Sobrescribir el archivo original con las solicitudes filtradas\n",
    "    df_filtered.to_csv(csv_file_path, index=False)\n",
    "\n",
    "    return total_rows_before, rows_removed\n",
    "\n",
    "# Llamar a la función para filtrar las solicitudes\n",
    "total_rows_before, rows_removed = filter_requests_with_userid(csv_file_path)\n",
    "\n",
    "# Imprimir la cantidad de filas eliminadas\n",
    "print(f\"Se eliminaron {rows_removed} filas sin userID de {total_rows_before}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff29ec51",
   "metadata": {},
   "source": [
    "## Cantidad de usuarios a dentro del rango de fecha evaluado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e7e267b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La cantidad de usuarios únicos es: 99785\n"
     ]
    }
   ],
   "source": [
    "# Leer el archivo CSV en un DataFrame\n",
    "df = pd.read_csv(csv_file_path, parse_dates=['date'])\n",
    "\n",
    "# Obtener la cantidad de usuarios únicos\n",
    "unique_users_count = df['userID'].nunique()\n",
    "\n",
    "# Imprimir la cantidad de usuarios únicos\n",
    "print(f\"La cantidad de usuarios únicos es: {unique_users_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8f1824",
   "metadata": {},
   "source": [
    "## Excluir puntos que esten dentro de lagos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b6bb1c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>origin_latitude</th>\n",
       "      <th>origin_longitude</th>\n",
       "      <th>destination_latitude</th>\n",
       "      <th>destination_longitude</th>\n",
       "      <th>userID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-09-12 09:55:24</td>\n",
       "      <td>-17.399618</td>\n",
       "      <td>-66.161091</td>\n",
       "      <td>-17.382153</td>\n",
       "      <td>-66.165591</td>\n",
       "      <td>680c28ba-5008-4ffe-8026-3eaad4f832d1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-09-12 09:55:32</td>\n",
       "      <td>-17.379989</td>\n",
       "      <td>-66.167222</td>\n",
       "      <td>-17.390862</td>\n",
       "      <td>-66.159421</td>\n",
       "      <td>680c28ba-5008-4ffe-8026-3eaad4f832d1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-09-12 10:05:28</td>\n",
       "      <td>-17.390862</td>\n",
       "      <td>-66.159421</td>\n",
       "      <td>-17.379989</td>\n",
       "      <td>-66.167222</td>\n",
       "      <td>680c28ba-5008-4ffe-8026-3eaad4f832d1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-09-12 10:07:36</td>\n",
       "      <td>-17.373542</td>\n",
       "      <td>-66.165138</td>\n",
       "      <td>-17.383765</td>\n",
       "      <td>-66.159304</td>\n",
       "      <td>680c28ba-5008-4ffe-8026-3eaad4f832d1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-09-12 11:27:27</td>\n",
       "      <td>-17.383765</td>\n",
       "      <td>-66.159304</td>\n",
       "      <td>-17.373542</td>\n",
       "      <td>-66.165138</td>\n",
       "      <td>680c28ba-5008-4ffe-8026-3eaad4f832d1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1577849</th>\n",
       "      <td>2023-12-26 22:25:49</td>\n",
       "      <td>-17.388356</td>\n",
       "      <td>-66.197263</td>\n",
       "      <td>-17.377586</td>\n",
       "      <td>-66.058064</td>\n",
       "      <td>a4f158b9-1359-49a0-9d7f-d28f393727ee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1577850</th>\n",
       "      <td>2023-12-26 22:26:06</td>\n",
       "      <td>-17.418014</td>\n",
       "      <td>-66.160858</td>\n",
       "      <td>-17.401366</td>\n",
       "      <td>-66.182851</td>\n",
       "      <td>bb060471-bc27-4175-af11-301f0853bf33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1577851</th>\n",
       "      <td>2023-12-26 22:26:36</td>\n",
       "      <td>-17.412731</td>\n",
       "      <td>-66.159542</td>\n",
       "      <td>-17.383059</td>\n",
       "      <td>-66.159274</td>\n",
       "      <td>958041c4-c269-4603-9237-fb7b917f042b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1577852</th>\n",
       "      <td>2023-12-26 22:26:52</td>\n",
       "      <td>-17.412731</td>\n",
       "      <td>-66.159542</td>\n",
       "      <td>-17.369551</td>\n",
       "      <td>-66.174464</td>\n",
       "      <td>958041c4-c269-4603-9237-fb7b917f042b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1577853</th>\n",
       "      <td>2023-12-26 22:29:54</td>\n",
       "      <td>-17.388356</td>\n",
       "      <td>-66.197263</td>\n",
       "      <td>-17.377978</td>\n",
       "      <td>-66.060869</td>\n",
       "      <td>a4f158b9-1359-49a0-9d7f-d28f393727ee</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1577854 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date  origin_latitude  origin_longitude  \\\n",
       "0       2022-09-12 09:55:24       -17.399618        -66.161091   \n",
       "1       2022-09-12 09:55:32       -17.379989        -66.167222   \n",
       "2       2022-09-12 10:05:28       -17.390862        -66.159421   \n",
       "3       2022-09-12 10:07:36       -17.373542        -66.165138   \n",
       "4       2022-09-12 11:27:27       -17.383765        -66.159304   \n",
       "...                     ...              ...               ...   \n",
       "1577849 2023-12-26 22:25:49       -17.388356        -66.197263   \n",
       "1577850 2023-12-26 22:26:06       -17.418014        -66.160858   \n",
       "1577851 2023-12-26 22:26:36       -17.412731        -66.159542   \n",
       "1577852 2023-12-26 22:26:52       -17.412731        -66.159542   \n",
       "1577853 2023-12-26 22:29:54       -17.388356        -66.197263   \n",
       "\n",
       "         destination_latitude  destination_longitude  \\\n",
       "0                  -17.382153             -66.165591   \n",
       "1                  -17.390862             -66.159421   \n",
       "2                  -17.379989             -66.167222   \n",
       "3                  -17.383765             -66.159304   \n",
       "4                  -17.373542             -66.165138   \n",
       "...                       ...                    ...   \n",
       "1577849            -17.377586             -66.058064   \n",
       "1577850            -17.401366             -66.182851   \n",
       "1577851            -17.383059             -66.159274   \n",
       "1577852            -17.369551             -66.174464   \n",
       "1577853            -17.377978             -66.060869   \n",
       "\n",
       "                                       userID  \n",
       "0        680c28ba-5008-4ffe-8026-3eaad4f832d1  \n",
       "1        680c28ba-5008-4ffe-8026-3eaad4f832d1  \n",
       "2        680c28ba-5008-4ffe-8026-3eaad4f832d1  \n",
       "3        680c28ba-5008-4ffe-8026-3eaad4f832d1  \n",
       "4        680c28ba-5008-4ffe-8026-3eaad4f832d1  \n",
       "...                                       ...  \n",
       "1577849  a4f158b9-1359-49a0-9d7f-d28f393727ee  \n",
       "1577850  bb060471-bc27-4175-af11-301f0853bf33  \n",
       "1577851  958041c4-c269-4603-9237-fb7b917f042b  \n",
       "1577852  958041c4-c269-4603-9237-fb7b917f042b  \n",
       "1577853  a4f158b9-1359-49a0-9d7f-d28f393727ee  \n",
       "\n",
       "[1577854 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(csv_file_path, parse_dates=['date'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c13b4083",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\idaas\\AppData\\Local\\Temp\\ipykernel_9136\\1986673735.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered.drop(['origin_point', 'destination_point'], axis=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Leer el archivo CSV en un DataFrame\n",
    "df = pd.read_csv(csv_file_path, parse_dates=['date'])\n",
    "\n",
    "# Crear puntos para las columnas de latitud y longitud\n",
    "df['origin_point'] = df.apply(lambda row: Point(row['origin_longitude'], row['origin_latitude']), axis=1)\n",
    "df['destination_point'] = df.apply(lambda row: Point(row['destination_longitude'], row['destination_latitude']), axis=1)\n",
    "\n",
    "# Crear un objeto multipolígono unión solo con los polígonos deseados\n",
    "desired_objectIds = [3635, 3486, 3204, 2948]\n",
    "\n",
    "shapes_reader = shapefile.Reader(lagos_datos)\n",
    "shapes = shapes_reader.shapes()\n",
    "records = shapes_reader.records()\n",
    "\n",
    "# Obtener información de las formas deseadas\n",
    "desired_shapes = [shape(shape_).buffer(0) for i, shape_ in enumerate(shapes) if records[i][0] in desired_objectIds]\n",
    "\n",
    "# Filtrar DataFrame para quedarse solo con los puntos que están fuera de los polígonos deseados\n",
    "df_filtered = df[~df['origin_point'].apply(lambda point: any(point.within(shape_) for shape_ in desired_shapes)) & ~df['destination_point'].apply(lambda point: any(point.within(shape_) for shape_ in desired_shapes))]\n",
    "# Guardar el DataFrame filtrado en un nuevo archivo CSV sin las columnas origin_point y destination_point\n",
    "df_filtered.drop(['origin_point', 'destination_point'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f058ccc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solicitudes antes del filtrado: 1577854\n",
      "Solicitudes después del filtrado: 1577111\n",
      "Las solicitudes filtradas se han guardado en: D:\\Trufiapp\\GANS\\Anexos\\Datos\\Registros de Trufi App\\r_origen-destino.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Contar líneas antes del filtrado\n",
    "rows_before = len(df)\n",
    "\n",
    "# Contar líneas después del filtrado\n",
    "rows_after = len(df_filtered)\n",
    "\n",
    "# Imprimir la cantidad de líneas antes y después del filtrado\n",
    "print(f\"Solicitudes antes del filtrado: {rows_before}\")\n",
    "print(f\"Solicitudes después del filtrado: {rows_after}\")\n",
    "\n",
    "# Guardar el DataFrame filtrado en un nuevo archivo CSV\n",
    "df_filtered.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"Las solicitudes filtradas se han guardado en: {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8968ff8f",
   "metadata": {},
   "source": [
    "## Excluir solicitudes de rutas con una distancia menor a 500 mts entre origen y destino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edda5f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer el archivo CSV en un DataFrame\n",
    "df = pd.read_csv(csv_file_path, parse_dates=['date'])\n",
    "\n",
    "# Crear puntos para las columnas de latitud y longitud\n",
    "df['origin_point'] = df.apply(lambda row: Point(row['origin_longitude'], row['origin_latitude']), axis=1)\n",
    "df['destination_point'] = df.apply(lambda row: Point(row['destination_longitude'], row['destination_latitude']), axis=1)\n",
    "\n",
    "# Calcular la distancia entre origen y destino en metros\n",
    "df['distancia'] = df.apply(lambda row: distance((row['origin_latitude'], row['origin_longitude']),\n",
    "                                               (row['destination_latitude'], row['destination_longitude'])).meters, axis=1)\n",
    "\n",
    "# Filtrar DataFrame para quedarse solo con los puntos que tienen una distancia mayor a 500 metros\n",
    "df_filtered = df[df['distancia'] > 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "331997c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solicitudes antes del filtrado: 1577111\n",
      "Solicitudes después del filtrado: 1446500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\idaas\\AppData\\Local\\Temp\\ipykernel_9136\\2112296885.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered.drop(['origin_point', 'destination_point'], axis=1, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las solicitudes filtradas se han guardado en: D:\\Trufiapp\\GANS\\Anexos\\Datos\\Registros de Trufi App\\r_origen-destino.csv\n"
     ]
    }
   ],
   "source": [
    "# Contar líneas antes del filtrado\n",
    "rows_before = len(df)\n",
    "\n",
    "# Contar líneas después del filtrado\n",
    "rows_after = len(df_filtered)\n",
    "\n",
    "# Imprimir la cantidad de líneas antes y después del filtrado\n",
    "print(f\"Solicitudes antes del filtrado: {rows_before}\")\n",
    "print(f\"Solicitudes después del filtrado: {rows_after}\")\n",
    "\n",
    "# Guardar el DataFrame filtrado en un nuevo archivo CSV sin las columnas origin_point y destination_point\n",
    "df_filtered.drop(['origin_point', 'destination_point'], axis=1, inplace=True)\n",
    "df_filtered.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"Las solicitudes filtradas se han guardado en: {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85afdc8d",
   "metadata": {},
   "source": [
    "## Generar variables para municipios origen y destino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7ef70dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer el archivo CSV en un DataFrame\n",
    "df = pd.read_csv(csv_file_path, parse_dates=['date'])\n",
    "    \n",
    "# Agregar las columnas 'origin_municipio' y 'dest_municipio' con valores predeterminados\n",
    "df['origin_municipio'] = 'externo'\n",
    "df['dest_municipio'] = 'externo'\n",
    "\n",
    "# Guardar el DataFrame actualizado en el archivo CSV\n",
    "df.to_csv(csv_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03dda4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "cochabamba_data = gpd.read_file(municipios_datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42548375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Trufiapp\\\\GANS\\\\Anexos\\\\Datos\\\\Registros de Trufi App\\\\r_origen-destino.csv'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def get_city_from_coords(coords):\n",
    "    point = Point(coords)\n",
    "    # Iterar sobre las características del GeoJSON\n",
    "    for index, row in cochabamba_data.iterrows():\n",
    "        city_geometry = row['geometry']\n",
    "        # Verificar si el punto está dentro de la geometría de la ciudad\n",
    "        if point.within(city_geometry):\n",
    "            return row['CAPITAL']\n",
    "    \n",
    "    return 'externo'\n",
    "\n",
    "# Agregar las columnas 'origin_municipio' y 'dest_municipio' al DataFrame\n",
    "columns_to_add = ['origin_municipio', 'dest_municipio']\n",
    "\n",
    "# Crear un DataFrame vacío con las nuevas columnas\n",
    "df = pd.DataFrame(columns=columns_to_add)\n",
    "# Abrir el archivo CSV para lectura y leer las columnas existentes\n",
    "with open(csv_file_path, 'r') as csvfile:\n",
    "    # Leer la primera línea para obtener los nombres de las columnas\n",
    "    existing_columns = csvfile.readline().strip().split(',')\n",
    "\n",
    "    # Crear un DataFrame vacío\n",
    "    df = pd.DataFrame(columns=existing_columns)\n",
    "\n",
    "    # Crear un archivo temporal para escribir las líneas actualizadas\n",
    "    with open('temp_csvfile.csv', 'w', newline='') as temp_csvfile:\n",
    "        # Escribir los nombres de las columnas en el archivo temporal\n",
    "        temp_csvfile.write(','.join(existing_columns) + '\\n')\n",
    "\n",
    "        for i, line in enumerate(csvfile):\n",
    "            # Leer una línea del archivo CSV\n",
    "            row = pd.read_csv(io.StringIO(line), header=None, names=existing_columns).iloc[0]\n",
    "\n",
    "            # Obtener nombres de municipios para origen y destino\n",
    "            origin_municipio = get_city_from_coords((row['origin_longitude'], row['origin_latitude']))\n",
    "            dest_municipio = get_city_from_coords((row['destination_longitude'], row['destination_latitude']))\n",
    "\n",
    "            # Agregar los valores al DataFrame\n",
    "            row['origin_municipio'] = origin_municipio\n",
    "            row['dest_municipio'] = dest_municipio\n",
    "\n",
    "            # Escribir la línea actualizada en el archivo temporal\n",
    "            temp_csvfile.write(','.join(map(str, row.values)) + '\\n')\n",
    "shutil.move('temp_csvfile.csv', csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4974d0fe",
   "metadata": {},
   "source": [
    "## Excluir solicitudes de rutas fuera de Cochabamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "108b296c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solicitudes antes del filtrado: 1446251\n",
      "Solicitudes después del filtrado: 1442497\n",
      "Las solicitudes filtradas se han guardado en: D:\\Trufiapp\\GANS\\Anexos\\Datos\\Registros de Trufi App\\origen-destino.csv\n"
     ]
    }
   ],
   "source": [
    "# Leer el archivo CSV en un DataFrame\n",
    "df = pd.read_csv(csv_file_path, parse_dates=['date'], encoding='latin1')\n",
    "# Filtrar el DataFrame para excluir las solicitudes con origen o destino \"externo\"\n",
    "df_filtered = df[(df['origin_municipio'] != 'externo') & (df['dest_municipio'] != 'externo')]\n",
    "\n",
    "# Contar líneas antes del filtrado\n",
    "rows_before = len(df)\n",
    "\n",
    "# Contar líneas después del filtrado\n",
    "rows_after = len(df_filtered)\n",
    "\n",
    "# Imprimir la cantidad de líneas antes y después del filtrado\n",
    "print(f\"Solicitudes antes del filtrado: {rows_before}\")\n",
    "print(f\"Solicitudes después del filtrado: {rows_after}\")\n",
    "\n",
    "# Guardar el DataFrame filtrado en un nuevo archivo CSV\n",
    "df_filtered.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"Las solicitudes filtradas se han guardado en: {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f09e2f",
   "metadata": {},
   "source": [
    "## Generar datos sinteticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e067bf24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       date  origin_latitude  origin_longitude  \\\n",
      "0       2022-09-12 09:55:24       -17.399618        -66.161091   \n",
      "1       2022-09-12 09:55:32       -17.379989        -66.167222   \n",
      "2       2022-09-12 10:05:28       -17.390862        -66.159421   \n",
      "3       2022-09-12 10:07:36       -17.373542        -66.165138   \n",
      "4       2022-09-12 11:27:27       -17.383765        -66.159304   \n",
      "...                     ...              ...               ...   \n",
      "1446246 2023-12-26 22:25:49       -17.388356        -66.197263   \n",
      "1446247 2023-12-26 22:26:06       -17.418014        -66.160858   \n",
      "1446248 2023-12-26 22:26:36       -17.412731        -66.159542   \n",
      "1446249 2023-12-26 22:26:52       -17.412731        -66.159542   \n",
      "1446250 2023-12-26 22:29:54       -17.388356        -66.197263   \n",
      "\n",
      "         destination_latitude  destination_longitude  \\\n",
      "0                  -17.382153             -66.165591   \n",
      "1                  -17.390862             -66.159421   \n",
      "2                  -17.379989             -66.167222   \n",
      "3                  -17.383765             -66.159304   \n",
      "4                  -17.373542             -66.165138   \n",
      "...                       ...                    ...   \n",
      "1446246            -17.377586             -66.058064   \n",
      "1446247            -17.401366             -66.182851   \n",
      "1446248            -17.383059             -66.159274   \n",
      "1446249            -17.369551             -66.174464   \n",
      "1446250            -17.377978             -66.060869   \n",
      "\n",
      "                                       userID     distancia origin_municipio  \\\n",
      "0        680c28ba-5008-4ffe-8026-3eaad4f832d1   1991.214699       Cochabamba   \n",
      "1        680c28ba-5008-4ffe-8026-3eaad4f832d1   1461.245383       Cochabamba   \n",
      "2        680c28ba-5008-4ffe-8026-3eaad4f832d1   1461.245383       Cochabamba   \n",
      "3        680c28ba-5008-4ffe-8026-3eaad4f832d1   1290.221915       Cochabamba   \n",
      "4        680c28ba-5008-4ffe-8026-3eaad4f832d1   1290.221915       Cochabamba   \n",
      "...                                       ...           ...              ...   \n",
      "1446246  a4f158b9-1359-49a0-9d7f-d28f393727ee  14840.186828       Cochabamba   \n",
      "1446247  bb060471-bc27-4175-af11-301f0853bf33   2975.792396       Cochabamba   \n",
      "1446248  958041c4-c269-4603-9237-fb7b917f042b   3284.052854       Cochabamba   \n",
      "1446249  958041c4-c269-4603-9237-fb7b917f042b   5035.099095       Cochabamba   \n",
      "1446250  a4f158b9-1359-49a0-9d7f-d28f393727ee  14539.593258       Cochabamba   \n",
      "\n",
      "        dest_municipio  hora  dia_de_semana  dia_de_mes  fin_de_semana  \n",
      "0           Cochabamba     9              0          12              0  \n",
      "1           Cochabamba     9              0          12              0  \n",
      "2           Cochabamba    10              0          12              0  \n",
      "3           Cochabamba    10              0          12              0  \n",
      "4           Cochabamba    11              0          12              0  \n",
      "...                ...   ...            ...         ...            ...  \n",
      "1446246         Sacaba    22              1          26              0  \n",
      "1446247     Cochabamba    22              1          26              0  \n",
      "1446248     Cochabamba    22              1          26              0  \n",
      "1446249     Cochabamba    22              1          26              0  \n",
      "1446250         Sacaba    22              1          26              0  \n",
      "\n",
      "[1446251 rows x 13 columns]\n",
      "Las solicitudes filtradas se han guardado en: D:\\Trufiapp\\GANS\\Anexos\\Datos\\Registros de Trufi App\\origen-destino.csv\n"
     ]
    }
   ],
   "source": [
    "# Agregar características de tiempo y distancia\n",
    "df['hora'] = df['date'].dt.hour\n",
    "df['dia_de_semana'] = df['date'].dt.dayofweek  # Lunes: 0, Domingo: 6\n",
    "df['dia_de_mes'] = df['date'].dt.day\n",
    "df['fin_de_semana'] = (df['date'].dt.weekday >= 5).astype(int)  # 1 si es fin de semana, 0 si no\n",
    "\n",
    "# Mostrar el DataFrame resultante\n",
    "print(df)\n",
    "\n",
    "# Guardar el DataFrame filtrado en un nuevo archivo CSV\n",
    "df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"Las solicitudes filtradas se han guardado en: {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd24a34b",
   "metadata": {},
   "source": [
    "## Integrar poligonos h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c4bc7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La información modificada se ha guardado en el archivo D:\\Trufiapp\\GANS\\Anexos\\Datos\\Registros de Trufi App\\origen-destino.csv.\n",
      "El máximo ID de destinos en Cochabamba es: 1113\n"
     ]
    }
   ],
   "source": [
    "# Función para obtener el índice H3 y las coordenadas\n",
    "def get_h3_index_and_coords(latitude, longitude, resolution=7):\n",
    "    h3_index = h3.geo_to_h3(latitude, longitude, resolution)\n",
    "    coords = h3.h3_to_geo(h3_index)\n",
    "    return h3_index, coords\n",
    "\n",
    "# Leer el archivo CSV en un DataFrame\n",
    "df = pd.read_csv(csv_file_path, parse_dates=['date'], encoding='latin1')\n",
    "\n",
    "# Crear columnas para índices H3 de origen y destino\n",
    "df['origin_h3_index'], df['origin_coords'] = zip(*df.apply(lambda row: get_h3_index_and_coords(row['origin_latitude'], row['origin_longitude']), axis=1))\n",
    "df['dest_h3_index'], df['dest_coords'] = zip(*df.apply(lambda row: get_h3_index_and_coords(row['destination_latitude'], row['destination_longitude']), axis=1))\n",
    "\n",
    "# Crear un diccionario que mapea cada índice H3 único a su propio ID único\n",
    "h3_index_to_id = {h3_index: idx + 1 for idx, h3_index in enumerate(pd.concat([df['origin_h3_index'], df['dest_h3_index']]).unique())}\n",
    "\n",
    "# Crear DataFrame con índices H3 únicos y sus IDs correspondientes y coordenadas\n",
    "h3_id_df = pd.DataFrame(list(h3_index_to_id.items()), columns=['h3_index', 'id'])\n",
    "h3_id_df[['latitude', 'longitude']] = pd.DataFrame(h3_id_df['h3_index'].apply(lambda h3_index: h3.h3_to_geo(h3_index)).to_list(), index=h3_id_df.index)\n",
    "\n",
    "# Guardar DataFrame de índices H3 y sus IDs en un archivo CSV separado\n",
    "h3_id_df.to_csv(h3_datos, index=False)\n",
    "\n",
    "# Asignar los IDs únicos tanto para el origen como para el destino utilizando el diccionario\n",
    "df['origin_id'] = df['origin_h3_index'].map(h3_index_to_id)\n",
    "df['destination_id'] = df['dest_h3_index'].map(h3_index_to_id)\n",
    "\n",
    "# Crear un nuevo DataFrame solo con las columnas necesarias\n",
    "new_df = df[['date','userID','distancia','origin_municipio','dest_municipio','hora','dia_de_semana','dia_de_mes','fin_de_semana','origin_h3_index','dest_h3_index','origin_id','destination_id']]\n",
    "\n",
    "new_df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"La información modificada se ha guardado en el archivo {csv_file_path}.\")\n",
    "max_origin_id = df['origin_id'].max()\n",
    "max_destination_id = df['destination_id'].max()\n",
    "\n",
    "print(f\"El máximo ID de destinos en Cochabamba es: {max_destination_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "032b30ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lista de IDs únicos de origin_id:\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804]\n",
      "Lista de IDs únicos de destination_id:\n",
      "[2, 3, 4, 805, 27, 806, 8, 23, 7, 29, 44, 14, 12, 11, 34, 17, 109, 1, 18, 255, 58, 55, 85, 71, 52, 5, 15, 26, 28, 75, 16, 9, 38, 30, 10, 35, 208, 24, 43, 19, 40, 13, 241, 50, 47, 51, 20, 102, 6, 53, 48, 31, 73, 99, 32, 62, 46, 83, 134, 56, 66, 135, 88, 45, 39, 101, 160, 97, 42, 41, 21, 468, 36, 61, 807, 771, 757, 54, 197, 33, 64, 350, 122, 275, 229, 80, 57, 113, 207, 136, 60, 110, 68, 283, 224, 86, 22, 82, 619, 91, 49, 132, 153, 93, 222, 115, 168, 179, 59, 146, 808, 183, 94, 218, 125, 145, 809, 810, 217, 238, 243, 78, 90, 69, 236, 130, 98, 155, 112, 346, 116, 157, 124, 129, 679, 192, 209, 142, 140, 811, 172, 133, 271, 74, 812, 813, 70, 210, 814, 273, 81, 815, 288, 193, 550, 87, 267, 148, 286, 269, 105, 240, 147, 344, 84, 280, 514, 191, 300, 174, 206, 144, 166, 670, 692, 816, 579, 803, 457, 801, 475, 259, 163, 817, 114, 226, 251, 558, 818, 182, 472, 187, 323, 819, 680, 216, 189, 239, 249, 199, 498, 820, 695, 159, 821, 756, 501, 232, 665, 198, 374, 237, 822, 599, 79, 467, 65, 200, 823, 76, 96, 824, 825, 310, 25, 366, 270, 214, 141, 826, 213, 827, 828, 170, 829, 37, 830, 143, 597, 212, 831, 804, 832, 833, 173, 118, 834, 792, 95, 321, 263, 592, 256, 595, 451, 835, 126, 231, 598, 668, 164, 836, 837, 201, 149, 656, 289, 838, 839, 123, 840, 484, 841, 842, 843, 158, 419, 261, 844, 477, 154, 845, 846, 847, 389, 848, 849, 415, 128, 292, 92, 279, 304, 850, 478, 851, 852, 247, 301, 302, 242, 853, 507, 854, 855, 299, 171, 382, 856, 857, 858, 348, 859, 235, 320, 860, 328, 684, 334, 335, 186, 106, 439, 861, 862, 863, 390, 864, 683, 865, 204, 413, 693, 866, 392, 352, 867, 868, 869, 870, 426, 547, 332, 281, 452, 871, 333, 872, 354, 361, 873, 77, 874, 875, 180, 876, 104, 313, 877, 377, 203, 169, 329, 878, 879, 741, 379, 880, 540, 417, 881, 524, 773, 882, 276, 883, 436, 151, 884, 885, 391, 336, 886, 887, 888, 889, 701, 890, 562, 891, 892, 202, 893, 894, 314, 453, 215, 895, 359, 896, 373, 897, 211, 898, 899, 900, 901, 414, 902, 903, 375, 904, 905, 906, 907, 427, 385, 908, 258, 909, 910, 425, 430, 911, 89, 435, 225, 341, 912, 119, 437, 913, 914, 659, 915, 916, 917, 587, 918, 919, 499, 920, 921, 922, 423, 923, 219, 924, 699, 925, 926, 927, 928, 519, 675, 929, 347, 930, 264, 331, 459, 931, 476, 932, 473, 483, 548, 489, 342, 933, 630, 474, 934, 935, 936, 325, 937, 938, 939, 687, 596, 940, 407, 488, 941, 942, 760, 943, 944, 945, 131, 946, 330, 947, 397, 948, 949, 503, 63, 950, 573, 951, 952, 953, 465, 954, 290, 510, 955, 956, 957, 958, 959, 569, 960, 961, 962, 963, 748, 713, 964, 513, 308, 262, 965, 966, 967, 523, 968, 969, 970, 971, 107, 528, 702, 972, 774, 973, 541, 974, 975, 686, 976, 532, 977, 978, 979, 233, 284, 980, 565, 185, 615, 572, 234, 194, 981, 982, 190, 983, 984, 985, 986, 326, 162, 716, 987, 245, 556, 988, 356, 989, 990, 589, 991, 150, 274, 992, 277, 993, 994, 800, 303, 995, 996, 997, 998, 353, 408, 585, 999, 538, 1000, 1001, 156, 1002, 613, 1003, 1004, 601, 1005, 285, 67, 802, 1006, 623, 1007, 1008, 1009, 628, 570, 1010, 343, 1011, 1012, 500, 1013, 1014, 1015, 1016, 371, 518, 1017, 420, 635, 1018, 491, 655, 1019, 1020, 606, 1021, 1022, 1023, 643, 1024, 287, 1025, 1026, 1027, 642, 1028, 622, 1029, 661, 1030, 1031, 759, 662, 1032, 388, 796, 482, 520, 421, 707, 673, 1033, 1034, 1035, 726, 669, 1036, 1037, 674, 462, 649, 1038, 663, 479, 1039, 195, 1040, 1041, 1042, 1043, 1044, 749, 691, 1045, 1046, 1047, 682, 770, 1048, 396, 1049, 1050, 1051, 688, 753, 511, 1052, 750, 1053, 1054, 1055, 1056, 745, 1057, 1058, 708, 1059, 1060, 1061, 671, 406, 1062, 1063, 1064, 1065, 724, 588, 1066, 254, 1067, 666, 1068, 1069, 738, 1070, 761, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 121, 591, 1078, 1079, 740, 754, 411, 1080, 1081, 1082, 752, 1083, 1084, 794, 758, 278, 1085, 1086, 1087, 1088, 765, 751, 651, 1089, 485, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 384, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 652, 470, 1106, 1107, 1108, 1109, 1110, 120, 1111, 1112, 228, 782, 1113]\n"
     ]
    }
   ],
   "source": [
    "# Leer el archivo CSV en un DataFrame\n",
    "df = pd.read_csv(csv_file_path, parse_dates=['date'], encoding='latin1')\n",
    "\n",
    "# Obtener una lista de IDs únicos correspondientes a origin_id\n",
    "unique_origin_ids = df['origin_id'].unique().tolist()\n",
    "unique_destination_ids = df['destination_id'].unique().tolist()\n",
    "\n",
    "# Imprimir la lista de IDs únicos\n",
    "print(\"Lista de IDs únicos de origin_id:\")\n",
    "print(unique_origin_ids)\n",
    "print(\"Lista de IDs únicos de destination_id:\")\n",
    "print(unique_destination_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30c49e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h3_index</th>\n",
       "      <th>id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>878b2c8a1ffffff</td>\n",
       "      <td>1</td>\n",
       "      <td>-17.397907</td>\n",
       "      <td>-66.169306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>878b2c8aeffffff</td>\n",
       "      <td>2</td>\n",
       "      <td>-17.375229</td>\n",
       "      <td>-66.167926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>878b2c8a3ffffff</td>\n",
       "      <td>3</td>\n",
       "      <td>-17.387033</td>\n",
       "      <td>-66.148877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>878b2c8a0ffffff</td>\n",
       "      <td>4</td>\n",
       "      <td>-17.409708</td>\n",
       "      <td>-66.150256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>878b2c98affffff</td>\n",
       "      <td>5</td>\n",
       "      <td>-17.455051</td>\n",
       "      <td>-66.153015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          h3_index  id   latitude  longitude\n",
       "0  878b2c8a1ffffff   1 -17.397907 -66.169306\n",
       "1  878b2c8aeffffff   2 -17.375229 -66.167926\n",
       "2  878b2c8a3ffffff   3 -17.387033 -66.148877\n",
       "3  878b2c8a0ffffff   4 -17.409708 -66.150256\n",
       "4  878b2c98affffff   5 -17.455051 -66.153015"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the DataFrame with H3 indices and coordinates\n",
    "h3_id_df = pd.read_csv(h3_datos)\n",
    "\n",
    "# Calculate the average latitude and longitude\n",
    "avg_lat = h3_id_df['latitude'].mean()\n",
    "avg_lon = h3_id_df['longitude'].mean()\n",
    "\n",
    "# Create a map centered at the average coordinates\n",
    "m = folium.Map(location=[avg_lat, avg_lon], zoom_start=12)\n",
    "\n",
    "# Set to keep track of printed hexagons\n",
    "printed_hexagons = set()\n",
    "\n",
    "h3_id_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f655c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hexágonos impresos: 1113\n"
     ]
    }
   ],
   "source": [
    "# Load the DataFrame with H3 indices and coordinates\n",
    "h3_id_df = pd.read_csv(h3_datos)\n",
    "m = folium.Map(location=[-17.392695, -66.156850], zoom_start=12)\n",
    "\n",
    "# Set to keep track of printed hexagons\n",
    "printed_hexagons = set()\n",
    "\n",
    "# Function to add a polygon to the map with a label\n",
    "def add_polygon_with_label(m, hexagon, label):\n",
    "    folium.Polygon(\n",
    "        locations=hexagon,\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fill_color='blue',\n",
    "        fill_opacity=0.4,\n",
    "        tooltip=f'Label: {label}'  # Add tooltip with the label number\n",
    "    ).add_to(m)\n",
    "    printed_hexagons.add(label)  # Use 'label' as a unique identifier\n",
    "\n",
    "# Iterate over the records\n",
    "for idx, row in h3_id_df.iterrows():\n",
    "    # Get the coordinates and convert them to a tuple\n",
    "    coords = (row['latitude'], row['longitude'])\n",
    "    hexagon_coords = h3.h3_to_geo_boundary(row['h3_index'])  # Get hexagon coordinates\n",
    "\n",
    "    # Check if the hexagon has not been printed yet\n",
    "    if row['id'] not in printed_hexagons:  # Use 'id' as a unique identifier\n",
    "        add_polygon_with_label(m, hexagon_coords, row['id'])  # Use 'id' as a label\n",
    "\n",
    "m.save(map_cochabamba_file_path)\n",
    "# Imprimir los hexágonos impresos\n",
    "print(\"Hexágonos impresos:\", len(printed_hexagons))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5e7d72",
   "metadata": {},
   "source": [
    "# Convertir a set de datos temporales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "702d1a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-VOOKUKL:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findspark.init()\n",
    "findspark.find()\n",
    "sc = SparkContext.getOrCreate()\n",
    "# Primera sesión de Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Trufi\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"3g\") \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .config(\"spark.master\", \"local\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Configurar el número de particiones\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e6faf0",
   "metadata": {},
   "source": [
    "## Contar cuantas solicitudes de origen por zona y hora existen\n",
    "## we are replacing  Pickup_datetime_hourly by Travel_datetime_hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a5a6439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in the aggregated DataFrame: 209458\n"
     ]
    }
   ],
   "source": [
    "# DataFrame original\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Convert the timestamp to an hourly format \n",
    "df = df.withColumn(\"Travel_datetime_hourly\", date_format(col(\"date\").cast(\"timestamp\"), \"yyyy-MM-dd HH:00\"))\n",
    "\n",
    "# Create a trip count column\n",
    "df = df.withColumn(\"Trip_count\", lit(1))\n",
    "\n",
    "# Group by pickup datetime, location ID, and aggregate the trip count\n",
    "hourly_aggregated = df.groupby(['Travel_datetime_hourly', 'origin_id']).agg({'Trip_count': 'count'}).withColumnRenamed(\"count(Trip_count)\", \"Trips_count\")\n",
    "\n",
    "# Crear la columna única usando concat_ws\n",
    "hourly_aggregated = hourly_aggregated.select(\n",
    "    concat_ws('_', col(\"Travel_datetime_hourly\"), col(\"origin_id\")).alias(\"UniqueColumn\"),\n",
    "    col(\"Travel_datetime_hourly\"),\n",
    "    col(\"origin_id\"),\n",
    "    col(\"Trips_count\")\n",
    ")\n",
    "\n",
    "hourly_aggregated.orderBy(col(\"Travel_datetime_hourly\").desc())\n",
    "# Count the number of rows in the aggregated DataFrame\n",
    "print(\"Total rows in the aggregated DataFrame:\", hourly_aggregated.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aafe6bf4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------+--------------+----+-------------+----------+-------------+---------------+---------------+---------+--------------+\n",
      "|               date|              userID|         distancia|origin_municipio|dest_municipio|hora|dia_de_semana|dia_de_mes|fin_de_semana|origin_h3_index|  dest_h3_index|origin_id|destination_id|\n",
      "+-------------------+--------------------+------------------+----------------+--------------+----+-------------+----------+-------------+---------------+---------------+---------+--------------+\n",
      "|2022-09-12 09:55:24|680c28ba-5008-4ff...| 1991.214698938187|      Cochabamba|    Cochabamba|   9|            0|        12|            0|878b2c8a1ffffff|878b2c8aeffffff|        1|             2|\n",
      "|2022-09-12 09:55:32|680c28ba-5008-4ff...|1461.2453834316614|      Cochabamba|    Cochabamba|   9|            0|        12|            0|878b2c8aeffffff|878b2c8a3ffffff|        2|             3|\n",
      "|2022-09-12 10:05:28|680c28ba-5008-4ff...|1461.2453834316614|      Cochabamba|    Cochabamba|  10|            0|        12|            0|878b2c8a3ffffff|878b2c8aeffffff|        3|             2|\n",
      "|2022-09-12 10:07:36|680c28ba-5008-4ff...|1290.2219151819006|      Cochabamba|    Cochabamba|  10|            0|        12|            0|878b2c8aeffffff|878b2c8a3ffffff|        2|             3|\n",
      "|2022-09-12 11:27:27|680c28ba-5008-4ff...|1290.2219151819006|      Cochabamba|    Cochabamba|  11|            0|        12|            0|878b2c8a3ffffff|878b2c8aeffffff|        3|             2|\n",
      "|2022-09-12 11:27:34|680c28ba-5008-4ff...|1290.2219151819006|      Cochabamba|    Cochabamba|  11|            0|        12|            0|878b2c8aeffffff|878b2c8a3ffffff|        2|             3|\n",
      "|2022-09-12 18:26:30|80ad0aad-b825-43c...| 1894.633275882146|      Cochabamba|    Cochabamba|  18|            0|        12|            0|878b2c8a0ffffff|878b2c8a0ffffff|        4|             4|\n",
      "|2022-09-12 18:46:18|95dd42fa-2451-4e3...| 7318732.780424042|      Cochabamba|       externo|  18|            0|        12|            0|878b2c8a3ffffff|871aa0743ffffff|        3|           805|\n",
      "|2022-09-12 18:54:27|f8437e7e-039f-47d...| 4277.174107755568|      Cochabamba|    Cochabamba|  18|            0|        12|            0|878b2c8a3ffffff|878b2c8a4ffffff|        3|            27|\n",
      "|2022-09-12 18:54:42|f8437e7e-039f-47d...| 4277.174107755568|      Cochabamba|    Cochabamba|  18|            0|        12|            0|878b2c8a3ffffff|878b2c8a4ffffff|        3|            27|\n",
      "|2022-09-12 19:12:20|e7c88e95-6b74-4df...| 7985.377161576159|      Cochabamba|    Cochabamba|  19|            0|        12|            0|878b2c98affffff|878b2c8aeffffff|        5|             2|\n",
      "|2022-09-12 19:13:24|e7c88e95-6b74-4df...| 7985.377161576159|      Cochabamba|    Cochabamba|  19|            0|        12|            0|878b2c98affffff|878b2c8aeffffff|        5|             2|\n",
      "|2022-09-12 19:13:29|e7c88e95-6b74-4df...| 7985.377161576159|      Cochabamba|    Cochabamba|  19|            0|        12|            0|878b2c98affffff|878b2c8aeffffff|        5|             2|\n",
      "|2022-09-12 23:08:33|3c90842f-f520-47b...| 7862.644057052703|      Cochabamba|    Cochabamba|  23|            0|        12|            0|878b2c998ffffff|878b2c8a3ffffff|        6|             3|\n",
      "|2022-09-12 23:34:57|680c28ba-5008-4ff...|1290.2219151819006|      Cochabamba|    Cochabamba|  23|            0|        12|            0|878b2c8aeffffff|878b2c8a3ffffff|        2|             3|\n",
      "|2022-09-12 23:35:39|3c90842f-f520-47b...| 7862.644057052703|      Cochabamba|    Cochabamba|  23|            0|        12|            0|878b2c998ffffff|878b2c8a3ffffff|        6|             3|\n",
      "|2022-09-12 23:35:47|3c90842f-f520-47b...| 9765.969947837784|      Cochabamba|    Cochabamba|  23|            0|        12|            0|878b2c998ffffff|878b2c8aeffffff|        6|             2|\n",
      "|2022-09-12 23:37:09|3c90842f-f520-47b...| 9765.969947837784|      Cochabamba|    Cochabamba|  23|            0|        12|            0|878b2c998ffffff|878b2c8aeffffff|        6|             2|\n",
      "|2022-09-13 07:45:01|9fdbda34-0608-4de...| 4083947.516404244|      Cochabamba|       externo|   7|            1|        13|            0|878b2c8aeffffff|87673299cffffff|        2|           806|\n",
      "|2022-09-13 09:02:39|680c28ba-5008-4ff...|1290.2219151819006|      Cochabamba|    Cochabamba|   9|            1|        13|            0|878b2c8a3ffffff|878b2c8aeffffff|        3|             2|\n",
      "+-------------------+--------------------+------------------+----------------+--------------+----+-------------+----------+-------------+---------------+---------------+---------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame original\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb245c3",
   "metadata": {},
   "source": [
    "## we are replacing  OriginLocationID by LocationID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "818f9f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+----------+------------------+\n",
      "|        UniqueColumn|               date|LocationID|         distancia|\n",
      "+--------------------+-------------------+----------+------------------+\n",
      "|2022-09-12 09:55:...|2022-09-12 09:55:24|         1| 1991.214698938187|\n",
      "|2022-09-12 09:55:...|2022-09-12 09:55:32|         2|1461.2453834316614|\n",
      "|2022-09-12 10:05:...|2022-09-12 10:05:28|         3|1461.2453834316614|\n",
      "|2022-09-12 10:07:...|2022-09-12 10:07:36|         2|1290.2219151819006|\n",
      "|2022-09-12 11:27:...|2022-09-12 11:27:27|         3|1290.2219151819006|\n",
      "|2022-09-12 11:27:...|2022-09-12 11:27:34|         2|1290.2219151819006|\n",
      "|2022-09-12 18:26:...|2022-09-12 18:26:30|         4| 1894.633275882146|\n",
      "|2022-09-12 18:46:...|2022-09-12 18:46:18|         3| 7318732.780424042|\n",
      "|2022-09-12 18:54:...|2022-09-12 18:54:27|         3| 4277.174107755568|\n",
      "|2022-09-12 18:54:...|2022-09-12 18:54:42|         3| 4277.174107755568|\n",
      "|2022-09-12 19:12:...|2022-09-12 19:12:20|         5| 7985.377161576159|\n",
      "|2022-09-12 19:13:...|2022-09-12 19:13:24|         5| 7985.377161576159|\n",
      "|2022-09-12 19:13:...|2022-09-12 19:13:29|         5| 7985.377161576159|\n",
      "|2022-09-12 23:08:...|2022-09-12 23:08:33|         6| 7862.644057052703|\n",
      "|2022-09-12 23:34:...|2022-09-12 23:34:57|         2|1290.2219151819006|\n",
      "|2022-09-12 23:35:...|2022-09-12 23:35:39|         6| 7862.644057052703|\n",
      "|2022-09-12 23:35:...|2022-09-12 23:35:47|         6| 9765.969947837784|\n",
      "|2022-09-12 23:37:...|2022-09-12 23:37:09|         6| 9765.969947837784|\n",
      "|2022-09-13 07:45:...|2022-09-13 07:45:01|         2| 4083947.516404244|\n",
      "|2022-09-13 09:02:...|2022-09-13 09:02:39|         3|1290.2219151819006|\n",
      "+--------------------+-------------------+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1446251"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.withColumn(\"LocationID\", col(\"origin_id\").cast(\"string\"))\n",
    "# Reemplaza \"df\" con tu DataFrame actual y \"origin_id_string\" con el nombre de tu nueva columna\n",
    "df = df.withColumn(\"UniqueColumn\", concat_ws('_', df[\"date\"], df[\"LocationID\"]))\n",
    "\n",
    "# Seleccionar las columnas deseadas\n",
    "df = df.select(\"UniqueColumn\", \"date\", \"LocationID\",\"distancia\")\n",
    "\n",
    "# Mostrar algunas filas para verificar\n",
    "df.show()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c99db246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11300</th>\n",
       "      <td>2023-12-26 20:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11301</th>\n",
       "      <td>2023-12-26 21:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11302</th>\n",
       "      <td>2023-12-26 22:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11303</th>\n",
       "      <td>2023-12-26 23:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11304</th>\n",
       "      <td>2023-12-27 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                timestamp\n",
       "11300 2023-12-26 20:00:00\n",
       "11301 2023-12-26 21:00:00\n",
       "11302 2023-12-26 22:00:00\n",
       "11303 2023-12-26 23:00:00\n",
       "11304 2023-12-27 00:00:00"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_series(start, stop, interval):\n",
    "    start = pd.Timestamp(start)\n",
    "    stop = pd.Timestamp(stop)\n",
    "    timestamps = pd.date_range(start, stop, freq=f'{interval}s')\n",
    "    return pd.DataFrame({'timestamp': timestamps})\n",
    "\n",
    "# Generar el DataFrame de marcas de tiempo\n",
    "timestamp_df = generate_series(\"2022-09-12\", \"2023-12-27\", 3600)  # Intervalo de 1 hora\n",
    "\n",
    "timestamp_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e2c536b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11304\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>hourly_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-09-12 00:00:00</th>\n",
       "      <td>2022-09-12 00:00:00</td>\n",
       "      <td>2022-09-12 00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-12 01:00:00</th>\n",
       "      <td>2022-09-12 01:00:00</td>\n",
       "      <td>2022-09-12 01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-12 02:00:00</th>\n",
       "      <td>2022-09-12 02:00:00</td>\n",
       "      <td>2022-09-12 02:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-12 03:00:00</th>\n",
       "      <td>2022-09-12 03:00:00</td>\n",
       "      <td>2022-09-12 03:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-12 04:00:00</th>\n",
       "      <td>2022-09-12 04:00:00</td>\n",
       "      <td>2022-09-12 04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-26 19:00:00</th>\n",
       "      <td>2023-12-26 19:00:00</td>\n",
       "      <td>2023-12-26 19:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-26 20:00:00</th>\n",
       "      <td>2023-12-26 20:00:00</td>\n",
       "      <td>2023-12-26 20:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-26 21:00:00</th>\n",
       "      <td>2023-12-26 21:00:00</td>\n",
       "      <td>2023-12-26 21:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-26 22:00:00</th>\n",
       "      <td>2023-12-26 22:00:00</td>\n",
       "      <td>2023-12-26 22:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-26 23:00:00</th>\n",
       "      <td>2023-12-26 23:00:00</td>\n",
       "      <td>2023-12-26 23:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11304 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              timestamp  hourly_timestamp\n",
       "2022-09-12 00:00:00 2022-09-12 00:00:00  2022-09-12 00:00\n",
       "2022-09-12 01:00:00 2022-09-12 01:00:00  2022-09-12 01:00\n",
       "2022-09-12 02:00:00 2022-09-12 02:00:00  2022-09-12 02:00\n",
       "2022-09-12 03:00:00 2022-09-12 03:00:00  2022-09-12 03:00\n",
       "2022-09-12 04:00:00 2022-09-12 04:00:00  2022-09-12 04:00\n",
       "...                                 ...               ...\n",
       "2023-12-26 19:00:00 2023-12-26 19:00:00  2023-12-26 19:00\n",
       "2023-12-26 20:00:00 2023-12-26 20:00:00  2023-12-26 20:00\n",
       "2023-12-26 21:00:00 2023-12-26 21:00:00  2023-12-26 21:00\n",
       "2023-12-26 22:00:00 2023-12-26 22:00:00  2023-12-26 22:00\n",
       "2023-12-26 23:00:00 2023-12-26 23:00:00  2023-12-26 23:00\n",
       "\n",
       "[11304 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generar el DataFrame de marcas de tiempo con intervalo de 1 hora\n",
    "timestamp_df = pd.date_range(\"2022-09-12\", \"2023-12-27\", freq='1H').to_frame(name='timestamp')\n",
    "\n",
    "# Aplicar el formato de fecha deseado\n",
    "timestamp_df['hourly_timestamp'] = timestamp_df['timestamp'].dt.strftime('%Y-%m-%d %H:00')\n",
    "# Eliminar la última fila del DataFrame\n",
    "timestamp_df = timestamp_df.iloc[:-1]\n",
    "\n",
    "# Verificar la longitud después de la eliminación\n",
    "print(len(timestamp_df))\n",
    "timestamp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c10629f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de filas en Pick_up_LocationID: 804\n"
     ]
    }
   ],
   "source": [
    "# Lee el archivo en un DataFrame de Spark\n",
    "id_index_df = spark.read.csv(h3_datos, header=True, inferSchema=True)\n",
    "\n",
    "# Encuentra los IDs de ubicación únicos del DataFrame original\n",
    "Pick_up_LocationID = id_index_df.select('id').distinct()\n",
    "Pick_up_LocationID = Pick_up_LocationID.orderBy(\"id\")\n",
    "Pick_up_LocationID = Pick_up_LocationID.withColumnRenamed(\"id\", \"LocationID\")\n",
    "# Convierte la columna 'LocationID' a tipo entero\n",
    "Pick_up_LocationID = Pick_up_LocationID.withColumn(\"LocationID\", Pick_up_LocationID[\"LocationID\"].cast(IntegerType()))\n",
    "\n",
    "# Convertir elementos de unique_origin_ids a enteros\n",
    "unique_origin_ids_int = [int(id) for id in unique_origin_ids]\n",
    "\n",
    "# Crear un DataFrame de Spark a partir de la lista unique_origin_ids_int\n",
    "row_list = [Row(LocationID=id) for id in unique_origin_ids_int]\n",
    "schema = [\"LocationID\"]\n",
    "Pick_up_LocationID = spark.createDataFrame(row_list, schema=schema)\n",
    "\n",
    "# Mostrar la cantidad de filas en el nuevo DataFrame\n",
    "print(\"Cantidad de filas en Pick_up_LocationID:\", Pick_up_LocationID.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7d1dee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define una función para agregar la columna locationID al DataFrame de timestamp\n",
    "def locations_timestamp_generator(dataframe, location_id):\n",
    "    dataframe[\"LocationID\"] = location_id\n",
    "    return dataframe\n",
    "\n",
    "# Define el esquema para el DataFrame final\n",
    "data_schema = [StructField(\"timestamp\", TimestampType(), True),\n",
    "               StructField(\"hourly_timestamp\", StringType(), True),\n",
    "               StructField(\"LocationID\", IntegerType(), True)]\n",
    "final_struct = StructType(fields=data_schema)\n",
    "# Crea un DataFrame vacío con las columnas requeridas\n",
    "data_schema = [\"timestamp\", \"hourly_timestamp\", \"LocationID\"]\n",
    "ID_plus_timestamp = pd.DataFrame(columns=data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1145ce03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp           9088416\n",
       "hourly_timestamp    9088416\n",
       "LocationID          9088416\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lista para almacenar DataFrames temporales\n",
    "frames = []\n",
    "\n",
    "# Itera sobre los IDs de ubicación y agrega las filas al DataFrame final\n",
    "for row in Pick_up_LocationID.collect():\n",
    "    location_id = row[\"LocationID\"]\n",
    "    timestamp_df_copy = timestamp_df.copy()\n",
    "    timestamp_df_copy = locations_timestamp_generator(timestamp_df_copy, location_id)\n",
    "    frames.append(timestamp_df_copy)\n",
    "\n",
    "# Concatena todos los DataFrames en la lista\n",
    "ID_plus_timestamp = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "# Muestra el resultado\n",
    "ID_plus_timestamp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2fb208d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de filas en el DataFrame resultante: 9088416\n",
      "                  timestamp  hourly_timestamp  LocationID\n",
      "9088411 2023-12-26 19:00:00  2023-12-26 19:00         804\n",
      "9088412 2023-12-26 20:00:00  2023-12-26 20:00         804\n",
      "9088413 2023-12-26 21:00:00  2023-12-26 21:00         804\n",
      "9088414 2023-12-26 22:00:00  2023-12-26 22:00         804\n",
      "9088415 2023-12-26 23:00:00  2023-12-26 23:00         804\n"
     ]
    }
   ],
   "source": [
    "print(\"Total de filas en el DataFrame resultante:\", len(ID_plus_timestamp))\n",
    "print(ID_plus_timestamp.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c66d2332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       UniqueColumn_G           timestamp  hourly_timestamp  LocationID\n",
      "0  2022-09-12 00:00_1 2022-09-12 00:00:00  2022-09-12 00:00           1\n",
      "1  2022-09-12 01:00_1 2022-09-12 01:00:00  2022-09-12 01:00           1\n",
      "2  2022-09-12 02:00_1 2022-09-12 02:00:00  2022-09-12 02:00           1\n",
      "3  2022-09-12 03:00_1 2022-09-12 03:00:00  2022-09-12 03:00           1\n",
      "4  2022-09-12 04:00_1 2022-09-12 04:00:00  2022-09-12 04:00           1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UniqueColumn_G      9088416\n",
       "timestamp           9088416\n",
       "hourly_timestamp    9088416\n",
       "LocationID          9088416\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cambiar el locationID de entero a cadena\n",
    "ID_plus_timestamp[\"locationID_string\"] = ID_plus_timestamp[\"LocationID\"].astype(str)\n",
    "\n",
    "# Crear la columna única utilizando concat_ws\n",
    "ID_plus_timestamp[\"UniqueColumn_G\"] = ID_plus_timestamp[\"hourly_timestamp\"] + \"_\" + ID_plus_timestamp[\"locationID_string\"]\n",
    "\n",
    "# Seleccionar las columnas relevantes\n",
    "ID_plus_timestamp = ID_plus_timestamp[[\"UniqueColumn_G\", \"timestamp\", \"hourly_timestamp\", \"LocationID\"]]\n",
    "\n",
    "# Mostrar algunas filas para verificar\n",
    "print(ID_plus_timestamp.head())\n",
    "ID_plus_timestamp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "82f1a7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 51756)\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\idaas\\anaconda3\\envs\\pyspark-env\\lib\\site-packages\\py4j\\clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"C:\\Users\\idaas\\anaconda3\\envs\\pyspark-env\\lib\\socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\idaas\\anaconda3\\envs\\pyspark-env\\lib\\site-packages\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"C:\\Users\\idaas\\anaconda3\\envs\\pyspark-env\\lib\\site-packages\\py4j\\clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\idaas\\anaconda3\\envs\\pyspark-env\\lib\\socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"C:\\Users\\idaas\\anaconda3\\envs\\pyspark-env\\lib\\socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"C:\\Users\\idaas\\anaconda3\\envs\\pyspark-env\\lib\\socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"C:\\Users\\idaas\\anaconda3\\envs\\pyspark-env\\lib\\socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"C:\\Users\\idaas\\anaconda3\\envs\\pyspark-env\\lib\\site-packages\\pyspark\\accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"C:\\Users\\idaas\\anaconda3\\envs\\pyspark-env\\lib\\site-packages\\pyspark\\accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"C:\\Users\\idaas\\anaconda3\\envs\\pyspark-env\\lib\\site-packages\\pyspark\\accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"C:\\Users\\idaas\\anaconda3\\envs\\pyspark-env\\lib\\site-packages\\pyspark\\serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "  File \"C:\\Users\\idaas\\anaconda3\\envs\\pyspark-env\\lib\\socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.readRDDFromFile",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m id_plus_timestamp \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mID_plus_timestamp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m hourly_aggregated\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pyspark-env\\lib\\site-packages\\pyspark\\sql\\session.py:1273\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m   1269\u001b[0m     data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data, columns\u001b[38;5;241m=\u001b[39mcolumn_names)\n\u001b[0;32m   1271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[0;32m   1272\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m-> 1273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSparkSession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\n\u001b[0;32m   1275\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1276\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(\n\u001b[0;32m   1277\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1278\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pyspark-env\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:440\u001b[0m, in \u001b[0;36mSparkConversionMixin.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    438\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    439\u001b[0m converted_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_from_pandas(data, schema, timezone)\n\u001b[1;32m--> 440\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconverted_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pyspark-env\\lib\\site-packages\\pyspark\\sql\\session.py:1318\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m   1316\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[0;32m   1317\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1318\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1320\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pyspark-env\\lib\\site-packages\\pyspark\\sql\\session.py:979\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[1;34m(self, data, schema)\u001b[0m\n\u001b[0;32m    977\u001b[0m \u001b[38;5;66;03m# convert python objects to sql data\u001b[39;00m\n\u001b[0;32m    978\u001b[0m internal_data \u001b[38;5;241m=\u001b[39m [struct\u001b[38;5;241m.\u001b[39mtoInternal(row) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m tupled_data]\n\u001b[1;32m--> 979\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallelize\u001b[49m\u001b[43m(\u001b[49m\u001b[43minternal_data\u001b[49m\u001b[43m)\u001b[49m, struct\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pyspark-env\\lib\\site-packages\\pyspark\\context.py:821\u001b[0m, in \u001b[0;36mSparkContext.parallelize\u001b[1;34m(self, c, numSlices)\u001b[0m\n\u001b[0;32m    818\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonParallelizeServer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc(), numSlices)\n\u001b[1;32m--> 821\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serialize_to_jvm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreader_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreateRDDServer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m RDD(jrdd, \u001b[38;5;28mself\u001b[39m, serializer)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pyspark-env\\lib\\site-packages\\pyspark\\context.py:867\u001b[0m, in \u001b[0;36mSparkContext._serialize_to_jvm\u001b[1;34m(self, data, serializer, reader_func, server_func)\u001b[0m\n\u001b[0;32m    865\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    866\u001b[0m         tempFile\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 867\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreader_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtempFile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# we eagerly reads the file so we can delete right after.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     os\u001b[38;5;241m.\u001b[39munlink(tempFile\u001b[38;5;241m.\u001b[39mname)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pyspark-env\\lib\\site-packages\\pyspark\\context.py:815\u001b[0m, in \u001b[0;36mSparkContext.parallelize.<locals>.reader_func\u001b[1;34m(temp_filename)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreader_func\u001b[39m(temp_filename: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JavaObject:\n\u001b[0;32m    814\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 815\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadRDDFromFile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumSlices\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pyspark-env\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pyspark-env\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pyspark-env\\lib\\site-packages\\py4j\\protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mPy4JError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.readRDDFromFile"
     ]
    }
   ],
   "source": [
    "id_plus_timestamp = spark.createDataFrame(ID_plus_timestamp.copy())\n",
    "hourly_aggregated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f99fd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambiar el ID de ubicación de entero a cadena\n",
    "id_plus_timestamp = id_plus_timestamp.withColumn(\"locationID_string\", col(\"LocationID\").cast(\"int\"))\n",
    "\n",
    "# Crear la columna única usando concat_ws\n",
    "id_plus_timestamp = id_plus_timestamp.select(\n",
    "    concat_ws('_', col(\"hourly_timestamp\"), col(\"locationID_string\")).alias(\"UniqueColumn_G\"),\n",
    "    col(\"timestamp\"),\n",
    "    col(\"hourly_timestamp\"),\n",
    "    col(\"LocationID\"),\n",
    "    col(\"hourly_timestamp\")\n",
    ")\n",
    "\n",
    "# Realizar la unión de ambos DataFrames usando la columna única\n",
    "pick_aggregated = hourly_aggregated.join(id_plus_timestamp, hourly_aggregated[\"UniqueColumn\"] == id_plus_timestamp[\"UniqueColumn_G\"], \"right\")\n",
    "# Mostrar algunas filas del resultado\n",
    "pick_aggregated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec03ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar solo las columnas deseadas\n",
    "columnas_deseadas = ['timestamp', 'hourly_timestamp', 'LocationID', 'Trips_count', 'UniqueColumn_G']\n",
    "pick_aggregated_o = pick_aggregated.select(*columnas_deseadas)\n",
    "\n",
    "# Ordenar por las columnas en el orden deseado\n",
    "pick_aggregated_o = pick_aggregated_o.orderBy(*[col(column) for column in columnas_deseadas])\n",
    "# Reemplazar los valores nulos en las columnas especificadas\n",
    "pick_aggregated_o = pick_aggregated_o.na.fill(0, \"Trips_count\")\n",
    "pick_aggregated_o = pick_aggregated_o.na.fill(0, \"LocationID\")\n",
    "\n",
    "# Cambiar el nombre de Trips_count a origin_request_count\n",
    "pick_aggregated_o = pick_aggregated_o.withColumnRenamed(\"Trips_count\", \"origin_request_count\")\n",
    "\n",
    "# Mostrar el DataFrame ordenado\n",
    "pick_aggregated_o.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83b1d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Especifica la ruta del archivo CSV en el sistema de archivos local\n",
    "csv_local_path = \"file:///\" + temporal_serie_data\n",
    "\n",
    "# Guardar el DataFrame en formato CSV en el sistema de archivos local\n",
    "pick_aggregated_o.write.mode(\"overwrite\").option(\"header\", \"true\").csv(csv_local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e1c6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lee el archivo CSV en un DataFrame\n",
    "df = spark.read.csv(\"file:///\" + temporal_serie_data, header=True, inferSchema=True)\n",
    "\n",
    "# Muestra el esquema del DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# Muestra las primeras filas del DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55fdb3e",
   "metadata": {},
   "source": [
    "## Contar cuantos destinos por zona y hora existen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24c1971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Convert the timestamp to an hourly format\n",
    "df = df.withColumn(\"Travel_datetime_hourly\", date_format(col(\"date\").cast(\"timestamp\"), \"yyyy-MM-dd HH:00\"))\n",
    "\n",
    "# Create a trip count column\n",
    "df = df.withColumn(\"Trip_count\", lit(1))\n",
    "\n",
    "# Group by pickup datetime, location ID, and aggregate the trip count\n",
    "hourly_aggregated = df.groupby(['Travel_datetime_hourly', 'destination_id']).agg({'Trip_count': 'count'}).withColumnRenamed(\"count(Trip_count)\", \"Trips_count\")\n",
    "\n",
    "# Crear la columna única usando concat_ws\n",
    "hourly_aggregated = hourly_aggregated.select(\n",
    "    concat_ws('_', col(\"Travel_datetime_hourly\"), col(\"destination_id\")).alias(\"UniqueColumn\"),\n",
    "    col(\"Travel_datetime_hourly\"),\n",
    "    col(\"destination_id\"),\n",
    "    col(\"Trips_count\")\n",
    ")\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "hourly_aggregated.show()\n",
    "\n",
    "# Count the number of rows in the aggregated DataFrame\n",
    "print(\"Total rows in the aggregated DataFrame:\", hourly_aggregated.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c46382f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realiza un left join de los DataFrames usando las columnas UniqueColumn y UniqueColumn_G\n",
    "merged_df = pick_aggregated_o.join(hourly_aggregated, pick_aggregated_o[\"UniqueColumn_G\"] == hourly_aggregated[\"UniqueColumn\"], \"left_outer\")\n",
    "\n",
    "# Selecciona las columnas deseadas en el DataFrame final\n",
    "final_merged = merged_df.select(\n",
    "    pick_aggregated_o[\"UniqueColumn_G\"],\n",
    "    pick_aggregated_o[\"hourly_timestamp\"],\n",
    "    pick_aggregated_o[\"LocationID\"],\n",
    "    hourly_aggregated[\"Trips_count\"].alias(\"destination_request_count\"),\n",
    "    pick_aggregated_o[\"origin_request_count\"]\n",
    ")\n",
    "\n",
    "# Muestra el DataFrame final\n",
    "final_merged.show()\n",
    "final_merged.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4f581d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merged = final_merged.na.fill(0, \"destination_request_count\")\n",
    "final_merged.orderBy(col(\"hourly_timestamp\").desc()).show(10)\n",
    "final_merged.show(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d545cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Year column\n",
    "final_merged = final_merged.withColumn('anio', year(final_merged['hourly_timestamp']))\n",
    "\n",
    "# Create Month column\n",
    "final_merged = final_merged.withColumn('mes', month(final_merged['hourly_timestamp']))\n",
    "\n",
    "# Create Day of Month column\n",
    "final_merged = final_merged.withColumn('dia_de_mes', dayofmonth(final_merged['hourly_timestamp']))\n",
    "\n",
    "# Create Hour column\n",
    "final_merged = final_merged.withColumn('hora', hour(final_merged['hourly_timestamp']))\n",
    "\n",
    "# Create Day of Week column\n",
    "final_merged = final_merged.withColumn(\"dia_de_semana\", dayofweek(col(\"hourly_timestamp\")))\n",
    "\n",
    "# Crear la columna isWeekend\n",
    "final_merged = final_merged.withColumn(\"fin_de_semana\", when((col(\"dia_de_semana\") == 1) | (col(\"dia_de_semana\") == 7), 1).otherwise(0))\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "final_merged.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511bc96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Especifica la ruta del archivo CSV en el sistema de archivos local\n",
    "csv_local_path = \"file:///\" + temporal_serie_data\n",
    "\n",
    "# Guardar el DataFrame en formato CSV en el sistema de archivos local\n",
    "final_merged.write.mode(\"overwrite\").option(\"header\", \"true\").csv(csv_local_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e944745e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "pyspark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
