{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "441311ee",
   "metadata": {},
   "source": [
    "## Importar bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b291dfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from shapely.geometry import shape, Point\n",
    "import geopandas as gpd\n",
    "from geopy.distance import distance\n",
    "import pandas as pd\n",
    "from h3 import h3\n",
    "from geopy.distance import geodesic\n",
    "import shapefile\n",
    "from shapely.geometry import Point, shape\n",
    "from shapely.ops import unary_union\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import col, lit, concat_ws, collect_set\n",
    "from pyspark.sql.types import TimestampType, StructField, StringType, IntegerType, StructType\n",
    "from pyspark.sql.functions import date_format\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "# from pyspark.ml.regression import LinearRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.sql import functions as F\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0966997b",
   "metadata": {},
   "source": [
    "## Obtener rutas de los Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf71b5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener la ruta de la carpeta del notebook\n",
    "notebook_folder = os.getcwd()\n",
    "root_project = os.path.abspath(os.path.join(notebook_folder, '..'))\n",
    "dataset_logs = os.path.abspath(os.path.join(root_project, 'Datos', 'Logs'))\n",
    "trufi_datos = os.path.abspath(os.path.join(root_project, 'Datos', 'Registros de Trufi App'))\n",
    "municipios_datos = os.path.abspath(os.path.join(root_project, 'Datos', 'Poligonos de Cochabamba','region_cochabamba_2018.geojson'))\n",
    "lagos_datos = os.path.abspath(os.path.join(root_project, 'Datos', 'Poligonos de Cochabamba','region_cochabamba_2018.geojson'))\n",
    "clima_datos = os.path.abspath(os.path.join(root_project, 'Datos', 'Datos del clima','weather.csv'))\n",
    "lagos_datos = os.path.abspath(os.path.join(root_project, 'Datos', 'Poligonos de Cochabamba','lagos.shx'))\n",
    "h3_datos = os.path.abspath(os.path.join(root_project, 'Datos', 'Registros de Trufi App','id_index_h3.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7234c941",
   "metadata": {},
   "source": [
    "## Extraer solicitudes de logs a origen-destino.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ed0542a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las rutas que cumplen con el patrón se han guardado en D:\\Trufiapp\\GANS\\Anexos\\Datos\\Registros de Trufi App\\origen-destino.csv.\n"
     ]
    }
   ],
   "source": [
    "# Definir la zona horaria de La Paz, Bolivia\n",
    "la_paz_timezone = pytz.timezone('America/La_Paz')\n",
    "def extract_route_info(log_line):\n",
    "    # Expresión regular para extraer información específica de las solicitudes de rutas\n",
    "    route_pattern_with_id = re.compile(r'GET /otp/plan\\?fromPlace=([-0-9.]+)%2C([-0-9.]+)&toPlace=([-0-9.]+)%2C([-0-9.]+).*?Trufi/.*?/([a-f0-9-]+)')\n",
    "    route_pattern_without_id = re.compile(r'GET /otp/plan\\?fromPlace=([-0-9.]+)%2C([-0-9.]+)&toPlace=([-0-9.]+)%2C([-0-9.]+)')\n",
    "\n",
    "    match_with_id = route_pattern_with_id.search(log_line)\n",
    "    match_without_id = route_pattern_without_id.search(log_line)\n",
    "\n",
    "    # Inicializar variables con valores predeterminados\n",
    "    origin_latitude = origin_longitude = dest_latitude = dest_longitude = id_user = None\n",
    "\n",
    "    try:\n",
    "        if match_with_id:\n",
    "            origin_latitude, origin_longitude, dest_latitude, dest_longitude, id_user = match_with_id.groups()\n",
    "        elif match_without_id:\n",
    "            origin_latitude, origin_longitude, dest_latitude, dest_longitude = match_without_id.groups()\n",
    "            id_user = 'N/A'  # Asignar un valor predeterminado\n",
    "        return origin_latitude, origin_longitude, dest_latitude, dest_longitude, id_user\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error al procesar la línea: {log_line}\")\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    return None\n",
    "def process_log_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'GET /otp/plan' in line:\n",
    "                date_str = re.search(r'\\[([^:]+:[^ ]+)', line).group(1)\n",
    "                \n",
    "                # Convertir la fecha a un objeto datetime y añadir la zona horaria de La Paz\n",
    "                date_time = datetime.strptime(date_str, '%d/%b/%Y:%H:%M:%S').replace(tzinfo=pytz.utc).astimezone(la_paz_timezone)\n",
    "\n",
    "                route_info = extract_route_info(line)\n",
    "                if route_info:\n",
    "                    origin_latitude, origin_longitude, dest_latitude, dest_longitude, id_user = route_info\n",
    "                    \n",
    "                    yield [date_time.strftime('%Y-%m-%d %H:%M:%S'),origin_latitude, origin_longitude, dest_latitude, dest_longitude, id_user]\n",
    "                    \n",
    "# Obtener la lista de archivos y ordenarlos según el prefijo numérico\n",
    "file_pattern = re.compile(r'^\\d{2}-')\n",
    "files = os.listdir(dataset_logs)\n",
    "log_files = [file for file in files if file_pattern.match(file)]\n",
    "log_files.sort(key=lambda x: int(x.split('-')[0]))\n",
    "\n",
    "# Guardar la información en un archivo CSV solo para rutas que cumplen con el patrón\n",
    "csv_file_path = os.path.join(trufi_datos, 'origen-destino.csv')\n",
    "header = ['date','origin_latitude', 'origin_longitude', 'destination_latitude', 'destination_longitude','userID']\n",
    "\n",
    "with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(header)\n",
    "\n",
    "    for log_file in log_files:\n",
    "        file_path = os.path.join(dataset_logs, log_file)\n",
    "        \n",
    "        route_info_generator = process_log_file(file_path)\n",
    "        \n",
    "        # Escribir las líneas en el archivo CSV\n",
    "        csv_writer.writerows(route_info_generator)\n",
    "\n",
    "print(f\"Las rutas que cumplen con el patrón se han guardado en {csv_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad63486",
   "metadata": {},
   "source": [
    "## Descartar solicitudes sin user ID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9c31deb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se eliminaron 83 filas sin userID de 1577937.\n"
     ]
    }
   ],
   "source": [
    "def filter_requests_with_userid(csv_file_path):\n",
    "    # Leer el archivo CSV\n",
    "    df = pd.read_csv(csv_file_path, parse_dates=['date'])\n",
    "\n",
    "    # Contar filas antes de la filtración\n",
    "    total_rows_before = len(df)\n",
    "\n",
    "    # Filtrar las solicitudes que tienen userID\n",
    "    df_filtered = df[df['userID'].notnull()]\n",
    "\n",
    "    # Contar filas después de la filtración\n",
    "    total_rows_after = len(df_filtered)\n",
    "\n",
    "    # Calcular cuántas filas fueron eliminadas\n",
    "    rows_removed = total_rows_before - total_rows_after\n",
    "\n",
    "    # Sobrescribir el archivo original con las solicitudes filtradas\n",
    "    df_filtered.to_csv(csv_file_path, index=False)\n",
    "\n",
    "    return total_rows_before, rows_removed\n",
    "\n",
    "# Llamar a la función para filtrar las solicitudes\n",
    "total_rows_before, rows_removed = filter_requests_with_userid(csv_file_path)\n",
    "\n",
    "# Imprimir la cantidad de filas eliminadas\n",
    "print(f\"Se eliminaron {rows_removed} filas sin userID de {total_rows_before}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d721fbd5",
   "metadata": {},
   "source": [
    "## Cantidad de usuarios a dentro del rango de fecha evaluado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d9629bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La cantidad de usuarios únicos es: 99785\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Obtener la cantidad de usuarios únicos\n",
    "unique_users_count = df['userID'].nunique()\n",
    "\n",
    "# Imprimir la cantidad de usuarios únicos\n",
    "print(f\"La cantidad de usuarios únicos es: {unique_users_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2867a9a",
   "metadata": {},
   "source": [
    "## Excluir puntos que esten dentro de lagos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a3df1c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>origin_latitude</th>\n",
       "      <th>origin_longitude</th>\n",
       "      <th>destination_latitude</th>\n",
       "      <th>destination_longitude</th>\n",
       "      <th>userID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-09-12 09:55:24</td>\n",
       "      <td>-17.399618</td>\n",
       "      <td>-66.161091</td>\n",
       "      <td>-17.382153</td>\n",
       "      <td>-66.165591</td>\n",
       "      <td>680c28ba-5008-4ffe-8026-3eaad4f832d1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-09-12 09:55:32</td>\n",
       "      <td>-17.379989</td>\n",
       "      <td>-66.167222</td>\n",
       "      <td>-17.390862</td>\n",
       "      <td>-66.159421</td>\n",
       "      <td>680c28ba-5008-4ffe-8026-3eaad4f832d1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-09-12 10:05:28</td>\n",
       "      <td>-17.390862</td>\n",
       "      <td>-66.159421</td>\n",
       "      <td>-17.379989</td>\n",
       "      <td>-66.167222</td>\n",
       "      <td>680c28ba-5008-4ffe-8026-3eaad4f832d1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-09-12 10:07:36</td>\n",
       "      <td>-17.373542</td>\n",
       "      <td>-66.165138</td>\n",
       "      <td>-17.383765</td>\n",
       "      <td>-66.159304</td>\n",
       "      <td>680c28ba-5008-4ffe-8026-3eaad4f832d1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-09-12 11:27:27</td>\n",
       "      <td>-17.383765</td>\n",
       "      <td>-66.159304</td>\n",
       "      <td>-17.373542</td>\n",
       "      <td>-66.165138</td>\n",
       "      <td>680c28ba-5008-4ffe-8026-3eaad4f832d1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1577849</th>\n",
       "      <td>2023-12-26 22:25:49</td>\n",
       "      <td>-17.388356</td>\n",
       "      <td>-66.197263</td>\n",
       "      <td>-17.377586</td>\n",
       "      <td>-66.058064</td>\n",
       "      <td>a4f158b9-1359-49a0-9d7f-d28f393727ee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1577850</th>\n",
       "      <td>2023-12-26 22:26:06</td>\n",
       "      <td>-17.418014</td>\n",
       "      <td>-66.160858</td>\n",
       "      <td>-17.401366</td>\n",
       "      <td>-66.182851</td>\n",
       "      <td>bb060471-bc27-4175-af11-301f0853bf33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1577851</th>\n",
       "      <td>2023-12-26 22:26:36</td>\n",
       "      <td>-17.412731</td>\n",
       "      <td>-66.159542</td>\n",
       "      <td>-17.383059</td>\n",
       "      <td>-66.159274</td>\n",
       "      <td>958041c4-c269-4603-9237-fb7b917f042b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1577852</th>\n",
       "      <td>2023-12-26 22:26:52</td>\n",
       "      <td>-17.412731</td>\n",
       "      <td>-66.159542</td>\n",
       "      <td>-17.369551</td>\n",
       "      <td>-66.174464</td>\n",
       "      <td>958041c4-c269-4603-9237-fb7b917f042b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1577853</th>\n",
       "      <td>2023-12-26 22:29:54</td>\n",
       "      <td>-17.388356</td>\n",
       "      <td>-66.197263</td>\n",
       "      <td>-17.377978</td>\n",
       "      <td>-66.060869</td>\n",
       "      <td>a4f158b9-1359-49a0-9d7f-d28f393727ee</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1577854 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date  origin_latitude  origin_longitude  \\\n",
       "0       2022-09-12 09:55:24       -17.399618        -66.161091   \n",
       "1       2022-09-12 09:55:32       -17.379989        -66.167222   \n",
       "2       2022-09-12 10:05:28       -17.390862        -66.159421   \n",
       "3       2022-09-12 10:07:36       -17.373542        -66.165138   \n",
       "4       2022-09-12 11:27:27       -17.383765        -66.159304   \n",
       "...                     ...              ...               ...   \n",
       "1577849 2023-12-26 22:25:49       -17.388356        -66.197263   \n",
       "1577850 2023-12-26 22:26:06       -17.418014        -66.160858   \n",
       "1577851 2023-12-26 22:26:36       -17.412731        -66.159542   \n",
       "1577852 2023-12-26 22:26:52       -17.412731        -66.159542   \n",
       "1577853 2023-12-26 22:29:54       -17.388356        -66.197263   \n",
       "\n",
       "         destination_latitude  destination_longitude  \\\n",
       "0                  -17.382153             -66.165591   \n",
       "1                  -17.390862             -66.159421   \n",
       "2                  -17.379989             -66.167222   \n",
       "3                  -17.383765             -66.159304   \n",
       "4                  -17.373542             -66.165138   \n",
       "...                       ...                    ...   \n",
       "1577849            -17.377586             -66.058064   \n",
       "1577850            -17.401366             -66.182851   \n",
       "1577851            -17.383059             -66.159274   \n",
       "1577852            -17.369551             -66.174464   \n",
       "1577853            -17.377978             -66.060869   \n",
       "\n",
       "                                       userID  \n",
       "0        680c28ba-5008-4ffe-8026-3eaad4f832d1  \n",
       "1        680c28ba-5008-4ffe-8026-3eaad4f832d1  \n",
       "2        680c28ba-5008-4ffe-8026-3eaad4f832d1  \n",
       "3        680c28ba-5008-4ffe-8026-3eaad4f832d1  \n",
       "4        680c28ba-5008-4ffe-8026-3eaad4f832d1  \n",
       "...                                       ...  \n",
       "1577849  a4f158b9-1359-49a0-9d7f-d28f393727ee  \n",
       "1577850  bb060471-bc27-4175-af11-301f0853bf33  \n",
       "1577851  958041c4-c269-4603-9237-fb7b917f042b  \n",
       "1577852  958041c4-c269-4603-9237-fb7b917f042b  \n",
       "1577853  a4f158b9-1359-49a0-9d7f-d28f393727ee  \n",
       "\n",
       "[1577854 rows x 6 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(csv_file_path, parse_dates=['date'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "55206ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creados\n"
     ]
    }
   ],
   "source": [
    "# Leer el archivo CSV en un DataFrame\n",
    "df = pd.read_csv(csv_file_path, parse_dates=['date'])\n",
    "\n",
    "# Crear puntos para las columnas de latitud y longitud\n",
    "df['origin_point'] = df.apply(lambda row: Point(row['origin_longitude'], row['origin_latitude']), axis=1)\n",
    "df['destination_point'] = df.apply(lambda row: Point(row['destination_longitude'], row['destination_latitude']), axis=1)\n",
    "\n",
    "# Crear un objeto multipolígono unión solo con los polígonos deseados\n",
    "desired_objectIds = [3635, 3486, 3204, 2948]\n",
    "print(f\"Creados\")\n",
    "shapes_reader = shapefile.Reader(lagos_datos)\n",
    "shapes = shapes_reader.shapes()\n",
    "records = shapes_reader.records()\n",
    "\n",
    "# Obtener información de las formas deseadas\n",
    "desired_shapes = [shape(shape_).buffer(0) for i, shape_ in enumerate(shapes) if records[i][0] in desired_objectIds]\n",
    "\n",
    "# Filtrar DataFrame para quedarse solo con los puntos que están fuera de los polígonos deseados\n",
    "df_filtered = df[~df['origin_point'].apply(lambda point: any(point.within(shape_) for shape_ in desired_shapes)) & ~df['destination_point'].apply(lambda point: any(point.within(shape_) for shape_ in desired_shapes))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f8170b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solicitudes antes del filtrado: 1577854\n",
      "Solicitudes después del filtrado: 1577111\n",
      "Las solicitudes filtradas se han guardado en: D:\\Trufiapp\\GANS\\Anexos\\Datos\\Registros de Trufi App\\origen-destino.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Contar líneas antes del filtrado\n",
    "rows_before = len(df)\n",
    "\n",
    "# Contar líneas después del filtrado\n",
    "rows_after = len(df_filtered)\n",
    "\n",
    "# Imprimir la cantidad de líneas antes y después del filtrado\n",
    "print(f\"Solicitudes antes del filtrado: {rows_before}\")\n",
    "print(f\"Solicitudes después del filtrado: {rows_after}\")\n",
    "\n",
    "# Guardar el DataFrame filtrado en un nuevo archivo CSV\n",
    "df_filtered.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"Las solicitudes filtradas se han guardado en: {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bcc923",
   "metadata": {},
   "source": [
    "## Excluir solicitudes de rutas con una distancia menor a 500 mts entre origen y destino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ae3c4416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer el archivo CSV en un DataFrame\n",
    "df = pd.read_csv(csv_file_path, parse_dates=['date'])\n",
    "\n",
    "# Crear puntos para las columnas de latitud y longitud\n",
    "df['origin_point'] = df.apply(lambda row: Point(row['origin_longitude'], row['origin_latitude']), axis=1)\n",
    "df['destination_point'] = df.apply(lambda row: Point(row['destination_longitude'], row['destination_latitude']), axis=1)\n",
    "\n",
    "# Calcular la distancia entre origen y destino en metros\n",
    "df['distancia'] = df.apply(lambda row: distance((row['origin_latitude'], row['origin_longitude']),\n",
    "                                               (row['destination_latitude'], row['destination_longitude'])).meters, axis=1)\n",
    "\n",
    "# Filtrar DataFrame para quedarse solo con los puntos que tienen una distancia mayor a 500 metros\n",
    "df_filtered = df[df['distancia'] > 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "110a7b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solicitudes antes del filtrado: 1577111\n",
      "Solicitudes después del filtrado: 1446500\n",
      "Las solicitudes filtradas se han guardado en: D:\\Trufiapp\\GANS\\Anexos\\Datos\\Registros de Trufi App\\origen-destino.csv\n"
     ]
    }
   ],
   "source": [
    "# Contar líneas antes del filtrado\n",
    "rows_before = len(df)\n",
    "\n",
    "# Contar líneas después del filtrado\n",
    "rows_after = len(df_filtered)\n",
    "\n",
    "# Imprimir la cantidad de líneas antes y después del filtrado\n",
    "print(f\"Solicitudes antes del filtrado: {rows_before}\")\n",
    "print(f\"Solicitudes después del filtrado: {rows_after}\")\n",
    "\n",
    "# Guardar el DataFrame filtrado en un nuevo archivo CSV sin las columnas origin_point y destination_point\n",
    "df_filtered.drop(['origin_point', 'destination_point'], axis=1, inplace=True)\n",
    "df_filtered.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"Las solicitudes filtradas se han guardado en: {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599d9ce5",
   "metadata": {},
   "source": [
    "## Generar variables para municipios origen y destino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccf9296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer el archivo CSV en un DataFrame\n",
    "df = pd.read_csv(csv_file_path, parse_dates=['date'])\n",
    "\n",
    "# Leer el archivo GeoJSON\n",
    "with open(municipios_datos) as f:\n",
    "    municipios_geojson = json.load(f)\n",
    "\n",
    "def get_city_from_coords(coords):\n",
    "    point = Point(coords)\n",
    "    # Iterar sobre las características del GeoJSON\n",
    "    for feature in municipios_geojson['features']:\n",
    "        municipio_geometry = shape(feature['geometry'])\n",
    "        # Verificar si el punto está dentro de la geometría del municipio\n",
    "        if point.within(municipio_geometry):\n",
    "            return feature['properties']['CAPITAL']\n",
    "    \n",
    "    return 'externo'\n",
    "\n",
    "# Ahora puedes usar esta función para asignar nombres de municipios a tus datos\n",
    "df['origin_municipio'] = df.apply(lambda row: get_city_from_coords((row['origin_longitude'], row['origin_latitude'])), axis=1)\n",
    "df['dest_municipio'] = df.apply(lambda row: get_city_from_coords((row['destination_longitude'], row['destination_latitude'])), axis=1)\n",
    "\n",
    "# Mostrar el DataFrame con los nombres de los municipios\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cb8cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el DataFrame filtrado en un nuevo archivo CSV\n",
    "df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"Las solicitudes filtradas se han guardado en: {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b87e73a",
   "metadata": {},
   "source": [
    "## Excluir solicitudes de rutas fuera de Cochabamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed5af48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer el archivo CSV en un DataFrame\n",
    "df = pd.read_csv(csv_file_path, parse_dates=['date'])\n",
    "\n",
    "# Filtrar el DataFrame para excluir las solicitudes con origen o destino \"externo\"\n",
    "df_filtered = df[(df['origin_municipio'] != 'externo') | (df['dest_municipio'] != 'externo')]\n",
    "\n",
    "# Contar líneas antes del filtrado\n",
    "rows_before = len(df)\n",
    "\n",
    "# Contar líneas después del filtrado\n",
    "rows_after = len(df_filtered)\n",
    "\n",
    "# Imprimir la cantidad de líneas antes y después del filtrado\n",
    "print(f\"Solicitudes antes del filtrado: {rows_before}\")\n",
    "print(f\"Solicitudes después del filtrado: {rows_after}\")\n",
    "\n",
    "# Guardar el DataFrame filtrado en un nuevo archivo CSV\n",
    "df_filtered.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"Las solicitudes filtradas se han guardado en: {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ce7d2e",
   "metadata": {},
   "source": [
    "## Generar datos sinteticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6e0620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar características de tiempo y distancia\n",
    "df['hora'] = df['date'].dt.hour\n",
    "df['dia_de_semana'] = df['date'].dt.dayofweek  # Lunes: 0, Domingo: 6\n",
    "df['dia_de_mes'] = df['date'].dt.day\n",
    "df['fin_de_semana'] = (df['date'].dt.weekday >= 5).astype(int)  # 1 si es fin de semana, 0 si no\n",
    "\n",
    "# Eliminar la columna 'date' si ya no es necesaria\n",
    "df = df.drop('date', axis=1)\n",
    "\n",
    "# Mostrar el DataFrame resultante\n",
    "print(df)\n",
    "\n",
    "\n",
    "# Guardar el DataFrame filtrado en un nuevo archivo CSV\n",
    "df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"Las solicitudes filtradas se han guardado en: {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feaf46f",
   "metadata": {},
   "source": [
    "## Integrar poligonos h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac52c335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para obtener el índice H3 y las coordenadas\n",
    "def get_h3_index_and_coords(latitude, longitude, resolution=7):\n",
    "    h3_index = h3.geo_to_h3(latitude, longitude, resolution)\n",
    "    coords = h3.h3_to_geo(h3_index)\n",
    "    return h3_index, coords\n",
    "\n",
    "# Leer el archivo CSV en un DataFrame\n",
    "df = pd.read_csv(csv_file_path, parse_dates=['date'], encoding='latin1')\n",
    "\n",
    "# Crear columnas para índices H3 de origen y destino\n",
    "df['origin_h3_index'], df['origin_coords'] = zip(*df.apply(lambda row: get_h3_index_and_coords(row['origin_latitude'], row['origin_longitude']), axis=1))\n",
    "df['dest_h3_index'], df['dest_coords'] = zip(*df.apply(lambda row: get_h3_index_and_coords(row['destination_latitude'], row['destination_longitude']), axis=1))\n",
    "\n",
    "# Crear un diccionario que mapea cada índice H3 único a su propio ID único\n",
    "h3_index_to_id = {h3_index: idx + 1 for idx, h3_index in enumerate(pd.concat([df['origin_h3_index'], df['dest_h3_index']]).unique())}\n",
    "\n",
    "# Crear DataFrame con índices H3 únicos y sus IDs correspondientes y coordenadas\n",
    "h3_id_df = pd.DataFrame(list(h3_index_to_id.items()), columns=['h3_index', 'id'])\n",
    "h3_id_df[['latitude', 'longitude']] = pd.DataFrame(h3_id_df['h3_index'].apply(lambda h3_index: h3.h3_to_geo(h3_index)).to_list(), index=h3_id_df.index)\n",
    "\n",
    "# Guardar DataFrame de índices H3 y sus IDs en un archivo CSV separado\n",
    "h3_id_df.to_csv(h3_datos, index=False)\n",
    "\n",
    "# Asignar los IDs únicos tanto para el origen como para el destino utilizando el diccionario\n",
    "df['origin_id'] = df['origin_h3_index'].map(h3_index_to_id)\n",
    "df['destination_id'] = df['dest_h3_index'].map(h3_index_to_id)\n",
    "\n",
    "# Crear un nuevo DataFrame solo con las columnas necesarias\n",
    "new_df = df[['date', 'hora', 'dia_de_semana', 'dia_de_mes', 'fin_de_semana', 'distancia', 'origin_id', 'destination_id']]\n",
    "\n",
    "# Guardar el nuevo DataFrame con IDs únicos y solo registros en Cochabamba en un nuevo directorio\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "new_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"La información modificada se ha guardado en el archivo {output_file_path}.\")\n",
    "max_origin_id = df['origin_id'].max()\n",
    "max_destination_id = df['destination_id'].max()\n",
    "\n",
    "# Obtener una lista de IDs únicos correspondientes a origin_id\n",
    "unique_origin_ids = df['origin_id'].unique().tolist()\n",
    "\n",
    "# Imprimir la lista de IDs únicos\n",
    "print(\"Lista de IDs únicos de origin_id:\")\n",
    "print(unique_origin_ids)\n",
    "print(f\"El máximo ID de destinos en Cochabamba es: {max_destination_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d959850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DataFrame with H3 indices and coordinates\n",
    "h3_id_df = pd.read_csv(h3_datos)\n",
    "\n",
    "# Calculate the average latitude and longitude\n",
    "avg_lat = h3_id_df['latitude'].mean()\n",
    "avg_lon = h3_id_df['longitude'].mean()\n",
    "\n",
    "# Create a map centered at the average coordinates\n",
    "m = folium.Map(location=[avg_lat, avg_lon], zoom_start=12)\n",
    "\n",
    "# Set to keep track of printed hexagons\n",
    "printed_hexagons = set()\n",
    "\n",
    "h3_id_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36059cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to add a polygon to the map with a label\n",
    "def add_polygon_with_label(m, hexagon, label):\n",
    "    folium.Polygon(\n",
    "        locations=hexagon,\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fill_color='blue',\n",
    "        fill_opacity=0.4,\n",
    "        tooltip=f'Label: {label}'  # Add tooltip with the label number\n",
    "    ).add_to(m)\n",
    "    printed_hexagons.add(label)  # Use 'label' as a unique identifier\n",
    "\n",
    "# Iterate over the records\n",
    "for idx, row in h3_id_df.iterrows():\n",
    "    # Get the coordinates and convert them to a tuple\n",
    "    coords = (row['latitude'], row['longitude'])\n",
    "    hexagon_coords = h3.h3_to_geo_boundary(row['h3_index'])  # Get hexagon coordinates\n",
    "\n",
    "    # Check if the hexagon has not been printed yet\n",
    "    if row['id'] not in printed_hexagons:  # Use 'id' as a unique identifier\n",
    "        add_polygon_with_label(m, hexagon_coords, row['id'])  # Use 'id' as a label\n",
    "\n",
    "# Save the map of origins and destinations in Cochabamba as an HTML file\n",
    "map_cochabamba_file_path = os.path.join(repo, 'h3_map_cochabamba_.html')\n",
    "m.save(map_cochabamba_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c3768c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimir los hexágonos impresos\n",
    "print(\"Hexágonos impresos:\", len(printed_hexagons))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd995771",
   "metadata": {},
   "source": [
    "## Normalizar fuentes de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702d1a05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5a6439",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "pyspark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
