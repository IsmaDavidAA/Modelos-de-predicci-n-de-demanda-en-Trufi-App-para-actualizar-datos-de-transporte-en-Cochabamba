{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bc99343",
   "metadata": {},
   "source": [
    "## Importar librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8c1395e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import col, lit, concat_ws, collect_set, from_unixtime, udf, array, sum, count\n",
    "from pyspark.sql.types import TimestampType, StructField, StringType, IntegerType, StructType, DoubleType\n",
    "from pyspark.sql.functions import date_format, col\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import holidays\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d7f25a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener la ruta de la carpeta del notebook\n",
    "notebook_folder = os.getcwd()\n",
    "root_project = os.path.abspath(os.path.join(notebook_folder, '..'))\n",
    "dataset_logs = os.path.abspath(os.path.join(root_project, 'Datos', 'Logs'))\n",
    "trufi_datos = os.path.abspath(os.path.join(root_project, 'Datos', 'Registros de Trufi App'))\n",
    "municipios_datos = os.path.abspath(os.path.join(root_project, 'Datos', 'Poligonos de Cochabamba','region_cochabamba_2018.geojson'))\n",
    "lagos_datos = os.path.abspath(os.path.join(root_project, 'Datos', 'Poligonos de Cochabamba','region_cochabamba_2018.geojson'))\n",
    "clima_datos = os.path.abspath(os.path.join(root_project, 'Datos', 'Datos del clima','weather.csv'))\n",
    "lagos_datos = os.path.abspath(os.path.join(root_project, 'Datos', 'Poligonos de Cochabamba','lagos.shx'))\n",
    "h3_datos = os.path.abspath(os.path.join(root_project, 'Datos', 'Registros de Trufi App','id_index_h3.csv'))\n",
    "map_cochabamba_file_path =  os.path.abspath(os.path.join(root_project, 'Datos', 'mapas','h3_map_cochabamba_.html'))\n",
    "temporal_serie_data = os.path.abspath(os.path.join(root_project, 'Datos', 'Registros de Trufi App','temporal_serie_data'))\n",
    "csv_file_path = os.path.join(trufi_datos, 'origen-destino.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c35f8961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-VOOKUKL:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findspark.init()\n",
    "findspark.find()\n",
    "sc = SparkContext.getOrCreate()\n",
    "# Primera sesión de Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Trufi\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"3g\") \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .config(\"spark.master\", \"local\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Configurar el número de particiones\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d290c917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Especifica la ruta del archivo CSV en el sistema de archivos local\n",
    "csv_local_path = \"file:///\" + temporal_serie_data\n",
    "# selected_columns = [\"UniqueColumn_G\", \"hourly_timestamp\", \"OriginLocationID\", \"destination_request_count\", \"origin_request_count\", \"Year\", \"Month\", \"DayOfMonth\", \"Hour\", \"DayOfWeek\", \"isWeekend\", \"id\"]\n",
    "# df_selected = df_final.select(selected_columns)\n",
    "# Guardar el DataFrame en formato CSV en el sistema de archivos local\n",
    "df_final.write.mode(\"overwrite\").option(\"header\", \"true\").csv(csv_local_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6fa9f95",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_final' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Agrupar por 'h3_index' y calcular el promedio de 'origin_request_count' para cada índice\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m average_trips_per_zone \u001b[38;5;241m=\u001b[39m \u001b[43mdf_final\u001b[49m\u001b[38;5;241m.\u001b[39mgroupBy(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh3_index\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39magg(avg(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morigin_request_count\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maverage_trips\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Ordenar por el promedio de viajes y tomar los 10 principales\u001b[39;00m\n\u001b[0;32m      5\u001b[0m top_10_zones \u001b[38;5;241m=\u001b[39m average_trips_per_zone\u001b[38;5;241m.\u001b[39morderBy(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maverage_trips\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mlimit(\u001b[38;5;241m60\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_final' is not defined"
     ]
    }
   ],
   "source": [
    "# Agrupar por 'h3_index' y calcular el promedio de 'origin_request_count' para cada índice\n",
    "average_trips_per_zone = df_final.groupBy('h3_index').agg(avg('origin_request_count').alias('average_trips'))\n",
    "\n",
    "# Ordenar por el promedio de viajes y tomar los 10 principales\n",
    "top_10_zones = average_trips_per_zone.orderBy('average_trips', ascending=False).limit(60)\n",
    "\n",
    "# Recopilar los datos de PySpark DataFrame y convertirlos a listas de Python\n",
    "h3_index_data = top_10_zones.select('h3_index').collect()\n",
    "average_trips_data = top_10_zones.select('average_trips').collect()\n",
    "\n",
    "# Extraer los valores de las listas\n",
    "h3_index_values = [row['h3_index'] for row in h3_index_data]\n",
    "average_trips_values = [row['average_trips'] for row in average_trips_data]\n",
    "\n",
    "# Crear el gráfico de barras horizontal\n",
    "fig, ax = plt.subplots(figsize=(20, 9))\n",
    "ax.barh(h3_index_values, average_trips_values)\n",
    "\n",
    "# Personalizar la visualización\n",
    "ax.set_xlabel('Número promedio de viajes')\n",
    "ax.set_ylabel('Índice H3')\n",
    "ax.set_title('Top 10 zonas por número promedio de viajes')\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792b72b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupar por 'h3_index' y calcular el promedio de 'origin_request_count' para cada índice\n",
    "average_trips_per_zone = df_final.groupBy('h3_index').agg(avg('destination_request_count').alias('average_trips'))\n",
    "\n",
    "# Ordenar por el promedio de viajes y tomar los 10 principales\n",
    "top_10_zones = average_trips_per_zone.orderBy('average_trips', ascending=False).limit(60)\n",
    "\n",
    "# Recopilar los datos de PySpark DataFrame y convertirlos a listas de Python\n",
    "h3_index_data = top_10_zones.select('h3_index').collect()\n",
    "average_trips_data = top_10_zones.select('average_trips').collect()\n",
    "\n",
    "# Extraer los valores de las listas\n",
    "h3_index_values = [row['h3_index'] for row in h3_index_data]\n",
    "average_trips_values = [row['average_trips'] for row in average_trips_data]\n",
    "\n",
    "# Crear el gráfico de barras horizontal\n",
    "fig, ax = plt.subplots(figsize=(20, 9))\n",
    "ax.barh(h3_index_values, average_trips_values)\n",
    "\n",
    "# Personalizar la visualización\n",
    "ax.set_xlabel('Número promedio de viajes')\n",
    "ax.set_ylabel('Índice H3')\n",
    "ax.set_title('Top 10 zonas por número promedio de viajes')\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b326e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupar por 'h3_index' y calcular la suma total y el recuento de solicitudes de origen y destino\n",
    "total_trips_per_zone = df_final.groupBy('h3_index', 'id').agg(sum('origin_request_count').alias('total_origin_trips'),\n",
    "                                                                    sum('destination_request_count').alias('total_destination_trips'),\n",
    "                                                                    count('*').alias('total_trips_count'))\n",
    "\n",
    "# Calcular la suma total de viajes sumando las solicitudes de origen y destino\n",
    "total_trips_per_zone = total_trips_per_zone.withColumn('total_trips', col('total_origin_trips') + col('total_destination_trips'))\n",
    "\n",
    "# Calcular el promedio dividiendo la suma total entre el recuento total\n",
    "total_trips_per_zone = total_trips_per_zone.withColumn('average_trips', col('total_trips') / col('total_trips_count'))\n",
    "\n",
    "# Ordenar por el promedio de viajes y tomar los 10 principales\n",
    "top_10_zones = total_trips_per_zone.orderBy('average_trips', ascending=False).limit(15)\n",
    "\n",
    "# Recopilar los datos de PySpark DataFrame y convertirlos a listas de Python\n",
    "h3_index_data = top_10_zones.select('h3_index', 'id').collect()\n",
    "average_trips_data = top_10_zones.select('average_trips').collect()\n",
    "\n",
    "# Extraer los valores de las listas\n",
    "h3_index_values = [f\"{row['h3_index']} (ID {row['id']})\" for row in h3_index_data]\n",
    "average_trips_values = [row['average_trips'] for row in average_trips_data]\n",
    "\n",
    "# Crear el gráfico de barras horizontal\n",
    "fig, ax = plt.subplots(figsize=(20, 9))\n",
    "ax.barh(h3_index_values, average_trips_values)\n",
    "\n",
    "# Personalizar la visualización\n",
    "ax.set_xlabel('Número promedio de viajes')\n",
    "ax.set_ylabel('Índice H3 (ID de Zona)')\n",
    "ax.set_title('Top 10 zonas por número promedio de viajes')\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bfbdf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "pyspark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
