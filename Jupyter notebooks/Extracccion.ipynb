{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "257011f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import re\n",
    "\n",
    "# repo = r'D:\\trufiapp\\GANS'\n",
    "# file_pattern = re.compile(r'^\\d{2}-')\n",
    "\n",
    "# # Obtener la lista de archivos en el directorio\n",
    "# files = os.listdir(repo)\n",
    "\n",
    "# # Filtrar solo los archivos de registro que cumplen con el patrón\n",
    "# log_files = [file for file in files if file_pattern.match(file)]\n",
    "\n",
    "# # Ordenar los archivos según el prefijo numérico\n",
    "# log_files.sort(key=lambda x: int(x.split('-')[0]))\n",
    "\n",
    "# # Mostrar un resumen del contenido de cada archivo\n",
    "# for log_file in log_files:\n",
    "#     file_path = os.path.join(repo, log_file)\n",
    "    \n",
    "#     # Leemos solo las primeras líneas del archivo para hacer un resumen\n",
    "#     with open(file_path, 'r') as f:\n",
    "#         content_summary = ''.join(f.readlines()[:5])  # Puedes ajustar el número de líneas que deseas mostrar\n",
    "\n",
    "#     print(f\"Resumen de contenido de {log_file}:\\n{content_summary}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c78a2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d11fb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from shapely.geometry import shape, Point\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from h3 import h3\n",
    "from geopy.distance import geodesic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12feff94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La información de las rutas dentro de alguna capital de Cochabamba se ha guardado en D:\\trufiapp\\GANS\\route_info.csv.\n"
     ]
    }
   ],
   "source": [
    "# Definir la zona horaria de La Paz, Bolivia\n",
    "la_paz_timezone = pytz.timezone('America/La_Paz')\n",
    "\n",
    "repo = r'D:\\trufiapp\\GANS'\n",
    "file_pattern = re.compile(r'^\\d{2}-')\n",
    "\n",
    "# Cargar el GeoDataFrame con información de las capitales de Cochabamba\n",
    "cochabamba_data = gpd.read_file(r'D:\\Trufiapp\\ETL\\Dataset\\region_cochabamba_2018\\region_cochabamba_2018.geojson')\n",
    "\n",
    "# Obtener las geometrías de las capitales de Cochabamba\n",
    "cochabamba_geometries = cochabamba_data['geometry']\n",
    "\n",
    "def is_point_inside_cochabamba(coords):\n",
    "    point = Point(coords)\n",
    "    # Verificar si el punto está dentro de alguna de las capitales de Cochabamba\n",
    "    return any(point.within(geom) for geom in cochabamba_geometries)\n",
    "\n",
    "def get_city_from_coords(coords):\n",
    "    point = Point(coords)\n",
    "    # Iterar sobre las filas del GeoDataFrame\n",
    "    for index, row in cochabamba_data.iterrows():\n",
    "        city_geometry = row['geometry']\n",
    "        # Verificar si el punto está dentro de la geometría de la ciudad\n",
    "        if point.within(city_geometry):\n",
    "            return row['CAPITAL']\n",
    "\n",
    "    return 'N/A'\n",
    "\n",
    "def extract_route_info(log_line):\n",
    "    # Expresión regular para extraer información específica de las solicitudes de rutas\n",
    "    route_pattern_with_id = re.compile(r'GET /otp/plan\\?fromPlace=([-0-9.]+)%2C([-0-9.]+)&toPlace=([-0-9.]+)%2C([-0-9.]+).*?Trufi/.*?/([a-f0-9-]+)')\n",
    "    route_pattern_without_id = re.compile(r'GET /otp/plan\\?fromPlace=([-0-9.]+)%2C([-0-9.]+)&toPlace=([-0-9.]+)%2C([-0-9.]+)')\n",
    "\n",
    "    match_with_id = route_pattern_with_id.search(log_line)\n",
    "    match_without_id = route_pattern_without_id.search(log_line)\n",
    "\n",
    "    # Inicializar variables con valores predeterminados\n",
    "    origin_latitude = origin_longitude = dest_latitude = dest_longitude = id_user = None\n",
    "\n",
    "    try:\n",
    "        if match_with_id:\n",
    "            origin_latitude, origin_longitude, dest_latitude, dest_longitude, id_user = match_with_id.groups()\n",
    "        elif match_without_id:\n",
    "            origin_latitude, origin_longitude, dest_latitude, dest_longitude = match_without_id.groups()\n",
    "            id_user = 'N/A'  # Asignar un valor predeterminado\n",
    "\n",
    "        # Verificar si los puntos de origen y destino están dentro de alguna capital de Cochabamba\n",
    "        if origin_latitude is not None and origin_longitude is not None and dest_latitude is not None and dest_longitude is not None:\n",
    "            origin_coords = (float(origin_longitude), float(origin_latitude))\n",
    "            dest_coords = (float(dest_longitude), float(dest_latitude))\n",
    "\n",
    "            if is_point_inside_cochabamba(origin_coords) and is_point_inside_cochabamba(dest_coords):\n",
    "                # Obtener la ciudad del origen y destino\n",
    "                origin_city = get_city_from_coords(origin_coords)\n",
    "                dest_city = get_city_from_coords(dest_coords)\n",
    "                # Calcular la distancia entre los puntos de origen y destino en metros\n",
    "                distance_meters = int(geodesic(origin_coords, dest_coords).meters)\n",
    "\n",
    "                return origin_latitude, origin_longitude, dest_latitude, dest_longitude, origin_city, dest_city, id_user, distance_meters\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error al procesar la línea: {log_line}\")\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "def process_log_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'GET /otp/plan' in line:\n",
    "                date_str = re.search(r'\\[([^:]+:[^ ]+)', line).group(1)\n",
    "\n",
    "                # Convertir la fecha a un objeto datetime y añadir la zona horaria de La Paz\n",
    "                date_time = datetime.strptime(date_str, '%d/%b/%Y:%H:%M:%S').replace(tzinfo=pytz.utc).astimezone(la_paz_timezone)\n",
    "                \n",
    "                # Obtener información adicional\n",
    "                hour = date_time.hour\n",
    "                day_of_week = date_time.weekday()  # 0: lunes, 1: martes, ..., 6: domingo\n",
    "                day_of_month = date_time.day\n",
    "                is_weekend = int(date_time.weekday() in [5, 6])  # 0: no es fin de semana, 1: es fin de semana\n",
    "\n",
    "                route_info = extract_route_info(line)\n",
    "                if route_info:\n",
    "                    origin_latitude, origin_longitude, dest_latitude, dest_longitude, origin_city, dest_city, id_user, distance_meters = route_info\n",
    "                    \n",
    "                    yield [date_time.strftime('%Y-%m-%d %H:%M:%S'), hour, day_of_week, day_of_month, is_weekend, origin_latitude, origin_longitude, dest_latitude, dest_longitude, origin_city, dest_city, id_user, distance_meters]\n",
    "\n",
    "# Obtener la lista de archivos y ordenarlos según el prefijo numérico\n",
    "files = os.listdir(repo)\n",
    "log_files = [file for file in files if file_pattern.match(file)]\n",
    "log_files.sort(key=lambda x: int(x.split('-')[0]))\n",
    "\n",
    "# Guardar la información en un archivo CSV\n",
    "csv_file_path = os.path.join(repo, 'route_info.csv')\n",
    "header = ['date_time', 'hour', 'day_of_week', 'day_of_month', 'is_weekend', 'origin_latitude', 'origin_longitude', 'destination_latitude', 'destination_longitude', 'origin_city', 'dest_city', 'id_user', 'distance_meters']\n",
    "\n",
    "with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(header)\n",
    "\n",
    "    for log_file in log_files:\n",
    "        file_path = os.path.join(repo, log_file)\n",
    "        route_info_generator = process_log_file(file_path)\n",
    "\n",
    "        # Escribir las líneas en el archivo CSV\n",
    "        csv_writer.writerows(route_info_generator)\n",
    "\n",
    "print(f\"La información de las rutas dentro de alguna capital de Cochabamba se ha guardado en {csv_file_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134c0d5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "227d0632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# repo = r'D:\\trufiapp\\GANS'\n",
    "# csv_file_path = os.path.join(repo, 'route_info.csv')\n",
    "\n",
    "# # Intenta leer el archivo CSV con una codificación diferente\n",
    "# try:\n",
    "#     df = pd.read_csv(csv_file_path, parse_dates=['date_time'], encoding='utf-8')\n",
    "# except UnicodeDecodeError:\n",
    "#     df = pd.read_csv(csv_file_path, parse_dates=['date_time'], encoding='latin-1')\n",
    "\n",
    "\n",
    "# # Crear un diccionario que mapea cada índice H3 único a su propio ID único para el origen\n",
    "# h3_index_to_origin_id = {h3_index: idx + 1 for idx, h3_index in enumerate(df['origin_h3_index'].unique())}\n",
    "\n",
    "# # Asignar los IDs únicos para el origen utilizando el diccionario\n",
    "# df['origin_id'] = df['origin_h3_index'].map(h3_index_to_origin_id)\n",
    "\n",
    "# # Crear un diccionario que mapea cada índice H3 único a su propio ID único para el destino\n",
    "# h3_index_to_dest_id = {h3_index: idx + 1 for idx, h3_index in enumerate(df['dest_h3_index'].unique())}\n",
    "\n",
    "# # Asignar los IDs únicos para el destino utilizando el diccionario\n",
    "# df['destination_id'] = df['dest_h3_index'].map(h3_index_to_dest_id)\n",
    "\n",
    "# # Crear un nuevo DataFrame solo con las columnas necesarias\n",
    "# new_df = df[['origin_id', 'origin_h3_index', 'destination_id', 'dest_h3_index']]\n",
    "\n",
    "# # Guardar el nuevo DataFrame con IDs únicos en columnas adyacentes en el mismo archivo CSV\n",
    "# df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "# print(f\"La información con IDs únicos se ha guardado en el mismo archivo {csv_file_path} en columnas adyacentes.\")\n",
    "# max_origin_id = df['origin_id'].max()\n",
    "# max_destination_id = df['destination_id'].max()\n",
    "\n",
    "# print(f\"El máximo ID de origenes es: {max_origin_id}\")\n",
    "# print(f\"El máximo ID de destinos es: {max_destination_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01eeff9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0e0c41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de filas y columnas después de filtrar: (950575, 15)\n"
     ]
    }
   ],
   "source": [
    "# Assuming df is your DataFrame\n",
    "df_filtered = df[df['distance_meters'] >= 500]\n",
    "# Imprimir el número de filas y columnas del DataFrame filtrado\n",
    "print(\"Número de filas y columnas después de filtrar:\", df_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "126303ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de filas y columnas después de filtrar: (950575, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\idaas\\AppData\\Local\\Temp\\ipykernel_4128\\2439089450.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['date_time'] = pd.to_datetime(df_filtered['date_time'])\n"
     ]
    }
   ],
   "source": [
    "df_filtered['date_time'] = pd.to_datetime(df_filtered['date_time'])\n",
    "\n",
    "# Filtra las filas con fechas mayores o iguales a '2022-09-09'\n",
    "df_filtered = df_filtered[df_filtered['date_time'] >= '2022-09-09']\n",
    "\n",
    "# Imprime el número de filas y columnas después de aplicar los filtros\n",
    "print(\"Número de filas y columnas después de filtrar:\", df_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "876104f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7120ad50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Supongamos que ID_plus_timestamp y hourly_aggregated son tus DataFrames\n",
    "# Pick_aggregated = pd.DataFrame()\n",
    "\n",
    "# # Número de filas por lote\n",
    "# batch_size = 10000\n",
    "\n",
    "# # Iterar sobre lotes\n",
    "# for i in range(0, len(ID_plus_timestamp), batch_size):\n",
    "#     # Obtener un lote del DataFrame ID_plus_timestamp\n",
    "#     batch_id_plus_timestamp = ID_plus_timestamp.iloc[i:i+batch_size]\n",
    "    \n",
    "#     # Realizar el merge con el lote actual\n",
    "#     merged_batch = pd.merge(hourly_aggregated, batch_id_plus_timestamp, left_on='UniqueColumn', right_on='UniqueColumn_G', how='right')\n",
    "    \n",
    "#     # Agregar el resultado parcial al DataFrame final\n",
    "#     Pick_aggregated = pd.concat([Pick_aggregated, merged_batch])\n",
    "\n",
    "# # Reemplazar valores nulos en las columnas PULocationID y Trips_count con 0\n",
    "# Pick_aggregated['PULocationID'].fillna(0, inplace=True)\n",
    "# Pick_aggregated['Trips_count'].fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d166d321",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149af781",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab01229d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c909e359",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d07a799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c339464e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El mapa con hexágonos H3 se ha guardado en D:\\trufiapp\\GANS\\h3_map.html.\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import h3\n",
    "# import folium\n",
    "# import pandas as pd\n",
    "\n",
    "# repo = r'D:\\trufiapp\\GANS'\n",
    "# csv_file_path = os.path.join(repo, 'route_info - Copy.csv')\n",
    "\n",
    "# # Cargar datos del archivo CSV en un DataFrame de Pandas\n",
    "# df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# # Crear un mapa con folium\n",
    "# m = folium.Map(location=[df['origin_latitude'].mean(), df['origin_longitude'].mean()], zoom_start=10)\n",
    "\n",
    "# # Agregar hexágonos al mapa para origenes\n",
    "# for _, row in df.iterrows():\n",
    "#     lat, lon = row['origin_latitude'], row['origin_longitude']\n",
    "#     hexagon = h3.h3_to_geo_boundary(row['origin_h3_index'])\n",
    "#     folium.Polygon(locations=hexagon, color='blue', fill=True, fill_color='blue', fill_opacity=0.4).add_to(m)\n",
    "\n",
    "# # Guardar el mapa de origenes como un archivo HTML\n",
    "# map_origin_file_path = os.path.join(repo, 'h3_map_origin.html')\n",
    "# m.save(map_origin_file_path)\n",
    "\n",
    "# # Crear un nuevo mapa con folium para destinos\n",
    "# m_dest = folium.Map(location=[df['destination_latitude'].mean(), df['destination_longitude'].mean()], zoom_start=10)\n",
    "\n",
    "# # Agregar hexágonos al mapa para destinos\n",
    "# for _, row in df.iterrows():\n",
    "#     lat, lon = row['destination_latitude'], row['destination_longitude']\n",
    "#     hexagon = h3.h3_to_geo_boundary(row['dest_h3_index'])\n",
    "#     folium.Polygon(locations=hexagon, color='red', fill=True, fill_color='red', fill_opacity=0.4).add_to(m_dest)\n",
    "\n",
    "# # Guardar el mapa de destinos como un archivo HTML\n",
    "# map_dest_file_path = os.path.join(repo, 'h3_map_dest.html')\n",
    "# m_dest.save(map_dest_file_path)\n",
    "\n",
    "# print(f\"Los mapas con hexágonos H3 para origenes y destinos se han guardado en {map_origin_file_path} y {map_dest_file_path} respectivamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b864630e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Leer tus datos\n",
    "# # Cambia 'ruta_de_tu_archivo.csv' al nombre de tu archivo\n",
    "# df = pd.read_csv('ruta_de_tu_archivo.csv')\n",
    "\n",
    "# # Seleccionar características y objetivo\n",
    "# features = ['hour', 'day_of_week', 'day_of_month', 'is_weekend', 'origin_id', 'destination_id']\n",
    "# target = 'distance_meters'\n",
    "\n",
    "# X = df[features]\n",
    "# y = df[target]\n",
    "\n",
    "# # Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# # Escalar los datos\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# # Entrenar un modelo de regresión lineal\n",
    "# linear_reg_model = LinearRegression()\n",
    "# linear_reg_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# # Realizar predicciones en el conjunto de prueba\n",
    "# y_pred = linear_reg_model.predict(X_test_scaled)\n",
    "\n",
    "# # Evaluar el rendimiento del modelo\n",
    "# model_metrics = get_metrics('Linear Regression', y_test, y_pred)\n",
    "\n",
    "# # Comparar métricas con otros modelos (puedes agregar más modelos según sea necesario)\n",
    "# compare_model_metrics([model_metrics])\n",
    "\n",
    "# # Visualizar algunas predicciones\n",
    "# plot_real_vs_pred('Linear Regression', y_test[:40], y_pred[:40])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "pyspark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
