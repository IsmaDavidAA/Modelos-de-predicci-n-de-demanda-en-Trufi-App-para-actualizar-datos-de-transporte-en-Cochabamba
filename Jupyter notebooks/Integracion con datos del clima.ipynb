{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bc99343",
   "metadata": {},
   "source": [
    "## Importar librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2a76b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Importar las bibliotecas necesarias\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import col, lit, concat_ws, collect_set, from_unixtime, date_format, udf, array, avg, sum, count\n",
    "from pyspark.sql.types import TimestampType, StructField, StringType, IntegerType, StructType, DoubleType\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import holidays\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238444f0",
   "metadata": {},
   "source": [
    "## Inicializar pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e22556e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-VOOKUKL:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findspark.init()\n",
    "findspark.find()\n",
    "# Importar las bibliotecas necesarias\n",
    "sc = SparkContext.getOrCreate()\n",
    "# Primera sesión de Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Trufi\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"3g\") \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .config(\"spark.master\", \"local\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Configurar el número de particiones\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963e85d6",
   "metadata": {},
   "source": [
    "## Obtener rutas de los Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e1c896d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener la ruta de la carpeta del notebook\n",
    "notebook_folder = os.getcwd()\n",
    "root_project = os.path.abspath(os.path.join(notebook_folder, '..'))\n",
    "dataset_logs = os.path.abspath(os.path.join(root_project, 'Datos', 'Logs'))\n",
    "trufi_datos = os.path.abspath(os.path.join(root_project, 'Datos', 'Registros de Trufi App'))\n",
    "municipios_datos = os.path.abspath(os.path.join(root_project, 'Datos', 'Poligonos de Cochabamba','region_cochabamba_2018.geojson'))\n",
    "lagos_datos = os.path.abspath(os.path.join(root_project, 'Datos', 'Poligonos de Cochabamba','region_cochabamba_2018.geojson'))\n",
    "clima_datos = os.path.abspath(os.path.join(root_project, 'Datos', 'Datos del clima','weather.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee32900",
   "metadata": {},
   "source": [
    "## Obtener rutas de los Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5981d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer el archivo CSV en un DataFrame\n",
    "df = pd.read_csv(csv_file_path, parse_dates=['date'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31ddf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lee el archivo CSV en un DataFrame\n",
    "df = spark.read.csv(\"file:///\" + out_agg_archivo_csv, header=True, inferSchema=True)\n",
    "\n",
    "# Muestra el esquema del DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# Muestra las primeras filas del DataFrame\n",
    "df.show()\n",
    "\n",
    "# Lee el archivo CSV en un DataFrame de Spark\n",
    "df_spark_result = spark.read.csv(weather_csv, header=True, inferSchema=True)\n",
    "\n",
    "# Selecciona solo las columnas necesarias del DataFrame de Spark resultante\n",
    "df_spark_selected = df_spark_result.select(\n",
    "    'datetime',\n",
    "    'temp',\n",
    "    'humidity',\n",
    "    'precip',\n",
    "    'cloudcover',\n",
    "    'windspeed'\n",
    ")\n",
    "\n",
    "# Renombra la columna 'hourly_timestamp' a 'datetime'\n",
    "df_spark_selected = df_spark_selected.withColumnRenamed('hourly_timestamp', 'datetime')\n",
    "\n",
    "# Muestra el esquema y las primeras filas del DataFrame de Spark resultante con las columnas seleccionadas\n",
    "df_spark_selected.printSchema()\n",
    "df_spark_selected.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3343508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define la fecha límite\n",
    "fecha_limite = '2023-12-27'\n",
    "\n",
    "# Filtra el DataFrame de Spark\n",
    "\n",
    "df_spark_result_filtered = df_spark_selected.filter(col('datetime') <= fecha_limite)\n",
    "df_spark_result_filtered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04015a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea la tabla temporal para df_spark_result_filtered\n",
    "df_spark_result_filtered.createOrReplaceTempView(\"df_spark_result_filtered\")\n",
    "\n",
    "# Clona el DataFrame df\n",
    "df_clone = df.select(*df.columns)\n",
    "\n",
    "# Realiza la fusión (merge) utilizando PySpark\n",
    "df_final = df_clone.join(\n",
    "    df_spark_result_filtered,\n",
    "    df_clone[\"hourly_timestamp\"] == df_spark_result_filtered[\"datetime\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Muestra el DataFrame resultante\n",
    "df_final.show(100)\n",
    "df_final.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a629106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un objeto para Bolivia\n",
    "bo_holidays = holidays.country_holidays('BO', subdiv='C', years=[2022, 2023])\n",
    "\n",
    "# Obtener los feriados para 2022 y 2023\n",
    "holidays_list = [(date, name) for date, name in sorted(bo_holidays.items())]\n",
    "\n",
    "# Crear un DataFrame con la lista de feriados\n",
    "holidays_df = pd.DataFrame(holidays_list, columns=[\"date\", \"holiday\"])\n",
    "# Creating new column contain 1 for the holiday\n",
    "holidays_df['Holiday_n']=1\n",
    "# Muestra los primeros registros del DataFrame\n",
    "print(holidays_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ca006f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear DataFrames de Spark desde los DataFrames de Pandas\n",
    "holidays_df_spark = spark.createDataFrame(holidays_df)\n",
    "\n",
    "# Realizar la unión utilizando Spark SQL\n",
    "df_final.createOrReplaceTempView(\"df_final\")\n",
    "holidays_df_spark.createOrReplaceTempView(\"holidays_df_spark\")\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT *\n",
    "    FROM df_final t\n",
    "    LEFT JOIN holidays_df_spark h\n",
    "    ON t.datetime = h.date\n",
    "\"\"\"\n",
    "df_final_merged_spark = spark.sql(query)\n",
    "df_final_merged_spark = df_final_merged_spark.drop(\"date\").drop(\"holiday\")\n",
    "\n",
    "# Reemplazar los valores nulos en la columna 'Holiday_n' con 0\n",
    "df_final_merged_spark = df_final_merged_spark.fillna(0, subset=[\"Holiday_n\"])\n",
    "\n",
    "# Muestra el DataFrame resultante\n",
    "df_final_merged_spark.count()\n",
    "df_final_merged_spark.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9630475c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar las columnas relevantes para el análisis de correlación\n",
    "selected_columns = [\"hourly_timestamp\",\"origin_request_count\", \"OriginLocationID\", \"destination_request_count\", \"Holiday_n\", \"windspeed\", \"cloudcover\", \"precip\", \"humidity\", \"temp\", \"isWeekend\",\"DayOfWeek\",\"Hour\",\"DayOfMonth\",\"Month\",\"Year\"]  # Agrega tus columnas aquí\n",
    "\n",
    "# Crear un DataFrame de Spark solo con las columnas seleccionadas\n",
    "selected_data = df_final_merged_spark.select(selected_columns)\n",
    "\n",
    "# Definir una función UDF para convertir las columnas en un vector\n",
    "vector_udf = udf(lambda arr: Vectors.dense(arr), VectorUDT())\n",
    "\n",
    "# Aplicar la conversión manualmente a las columnas seleccionadas\n",
    "for col_name in selected_columns:\n",
    "    selected_data = selected_data.withColumn(col_name, col(col_name).cast(DoubleType()))\n",
    "\n",
    "selected_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b2f965",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selected_data_p = selected_data.toPandas()\n",
    "# Filtra las filas que contienen al menos un valor NaN en cualquier columna\n",
    "condition = selected_data_p[\"temp\"].isna() | selected_data_p[\"precip\"].isna() | selected_data_p[\"humidity\"].isna() | selected_data_p[\"cloudcover\"].isna()\n",
    "df_final_with_nan = selected_data_p[condition]\n",
    "\n",
    "# Muestra el DataFrame resultante\n",
    "df_final_with_nan\n",
    "len(df_final_with_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806a0a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_data_p.corr()['origin_request_count'].sort_values().drop('origin_request_count').plot(kind='bar',figsize=(10,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b833ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "sns.heatmap(selected_data_p.corr(), linewidth=0.3, linecolor='black', cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e14c908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lee el archivo de H3 en un DataFrame de PySpark\n",
    "h3_df = spark.read.csv(\"D:\\Trufiapp\\GANS\\id_index_h3.csv\", header=True, inferSchema=True)\n",
    "# Convierte el DataFrame de Pandas a un DataFrame de PySpark\n",
    "selected_data_spark = spark.createDataFrame(selected_data_p)\n",
    "# Convierte la columna 'hourly_timestamp' a tipo fecha\n",
    "selected_data_spark = selected_data_spark.withColumn('hourly_timestamp', from_unixtime('hourly_timestamp').cast(TimestampType()))\n",
    "\n",
    "# Realiza la unión utilizando la condición de igualdad en las columnas\n",
    "df_final = selected_data_spark.join(h3_df, col(\"OriginLocationID\") == col(\"id\"), \"left\")\n",
    "df_final.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04cae27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.count()\n",
    "# Muestra los últimos elementos (por ejemplo, los últimos 10)\n",
    "df_final.orderBy(col(\"hourly_timestamp\").desc()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004b227b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Especifica la ruta del archivo CSV en el sistema de archivos local\n",
    "csv_local_path = \"file:///\" + datasetmerged\n",
    "# selected_columns = [\"UniqueColumn_G\", \"hourly_timestamp\", \"OriginLocationID\", \"destination_request_count\", \"origin_request_count\", \"Year\", \"Month\", \"DayOfMonth\", \"Hour\", \"DayOfWeek\", \"isWeekend\", \"id\"]\n",
    "# df_selected = df_final.select(selected_columns)\n",
    "# Guardar el DataFrame en formato CSV en el sistema de archivos local\n",
    "df_final.write.mode(\"overwrite\").option(\"header\", \"true\").csv(csv_local_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846caaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupar por 'h3_index' y calcular el promedio de 'origin_request_count' para cada índice\n",
    "average_trips_per_zone = df_final.groupBy('h3_index').agg(avg('origin_request_count').alias('average_trips'))\n",
    "\n",
    "# Ordenar por el promedio de viajes y tomar los 10 principales\n",
    "top_10_zones = average_trips_per_zone.orderBy('average_trips', ascending=False).limit(60)\n",
    "\n",
    "# Recopilar los datos de PySpark DataFrame y convertirlos a listas de Python\n",
    "h3_index_data = top_10_zones.select('h3_index').collect()\n",
    "average_trips_data = top_10_zones.select('average_trips').collect()\n",
    "\n",
    "# Extraer los valores de las listas\n",
    "h3_index_values = [row['h3_index'] for row in h3_index_data]\n",
    "average_trips_values = [row['average_trips'] for row in average_trips_data]\n",
    "\n",
    "# Crear el gráfico de barras horizontal\n",
    "fig, ax = plt.subplots(figsize=(20, 9))\n",
    "ax.barh(h3_index_values, average_trips_values)\n",
    "\n",
    "# Personalizar la visualización\n",
    "ax.set_xlabel('Número promedio de viajes')\n",
    "ax.set_ylabel('Índice H3')\n",
    "ax.set_title('Top 10 zonas por número promedio de viajes')\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303bd6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupar por 'h3_index' y calcular el promedio de 'origin_request_count' para cada índice\n",
    "average_trips_per_zone = df_final.groupBy('h3_index').agg(avg('destination_request_count').alias('average_trips'))\n",
    "\n",
    "# Ordenar por el promedio de viajes y tomar los 10 principales\n",
    "top_10_zones = average_trips_per_zone.orderBy('average_trips', ascending=False).limit(60)\n",
    "\n",
    "# Recopilar los datos de PySpark DataFrame y convertirlos a listas de Python\n",
    "h3_index_data = top_10_zones.select('h3_index').collect()\n",
    "average_trips_data = top_10_zones.select('average_trips').collect()\n",
    "\n",
    "# Extraer los valores de las listas\n",
    "h3_index_values = [row['h3_index'] for row in h3_index_data]\n",
    "average_trips_values = [row['average_trips'] for row in average_trips_data]\n",
    "\n",
    "# Crear el gráfico de barras horizontal\n",
    "fig, ax = plt.subplots(figsize=(20, 9))\n",
    "ax.barh(h3_index_values, average_trips_values)\n",
    "\n",
    "# Personalizar la visualización\n",
    "ax.set_xlabel('Número promedio de viajes')\n",
    "ax.set_ylabel('Índice H3')\n",
    "ax.set_title('Top 10 zonas por número promedio de viajes')\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e760c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupar por 'h3_index' y calcular la suma total y el recuento de solicitudes de origen y destino\n",
    "total_trips_per_zone = df_final.groupBy('h3_index', 'id').agg(sum('origin_request_count').alias('total_origin_trips'),\n",
    "                                                                    sum('destination_request_count').alias('total_destination_trips'),\n",
    "                                                                    count('*').alias('total_trips_count'))\n",
    "\n",
    "# Calcular la suma total de viajes sumando las solicitudes de origen y destino\n",
    "total_trips_per_zone = total_trips_per_zone.withColumn('total_trips', col('total_origin_trips') + col('total_destination_trips'))\n",
    "\n",
    "# Calcular el promedio dividiendo la suma total entre el recuento total\n",
    "total_trips_per_zone = total_trips_per_zone.withColumn('average_trips', col('total_trips') / col('total_trips_count'))\n",
    "\n",
    "# Ordenar por el promedio de viajes y tomar los 10 principales\n",
    "top_10_zones = total_trips_per_zone.orderBy('average_trips', ascending=False).limit(15)\n",
    "\n",
    "# Recopilar los datos de PySpark DataFrame y convertirlos a listas de Python\n",
    "h3_index_data = top_10_zones.select('h3_index', 'id').collect()\n",
    "average_trips_data = top_10_zones.select('average_trips').collect()\n",
    "\n",
    "# Extraer los valores de las listas\n",
    "h3_index_values = [f\"{row['h3_index']} (ID {row['id']})\" for row in h3_index_data]\n",
    "average_trips_values = [row['average_trips'] for row in average_trips_data]\n",
    "\n",
    "# Crear el gráfico de barras horizontal\n",
    "fig, ax = plt.subplots(figsize=(20, 9))\n",
    "ax.barh(h3_index_values, average_trips_values)\n",
    "\n",
    "# Personalizar la visualización\n",
    "ax.set_xlabel('Número promedio de viajes')\n",
    "ax.set_ylabel('Índice H3 (ID de Zona)')\n",
    "ax.set_title('Top 10 zonas por número promedio de viajes')\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c2eaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c1395e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "pyspark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
