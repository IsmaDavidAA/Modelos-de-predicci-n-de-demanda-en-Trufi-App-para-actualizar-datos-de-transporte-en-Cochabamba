{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a52d008-88a1-4f92-8f7c-cac8b3bf8514",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pip install h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "069d2a72-8c98-4153-b49b-180d7241120c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h3\n",
    "import folium\n",
    "import pandas as pd\n",
    "import ast  # Para convertir la cadena de coordenadas a una tupla\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8152bd95-6456-4c83-93b4-5c34aadca320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad313bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para obtener el índice H3 y las coordenadas\n",
    "def get_h3_index_and_coords(latitude, longitude, resolution=7):\n",
    "    h3_index = h3.geo_to_h3(latitude, longitude, resolution)\n",
    "    coords = h3.h3_to_geo(h3_index)\n",
    "    return h3_index, coords\n",
    "\n",
    "repo = r'D:\\trufiapp\\GANS'\n",
    "csv_file_path = os.path.join(repo, 'route_info.csv')\n",
    "csv_save_file_path = os.path.join(repo, 'id_index_h3.csv')\n",
    "output_dir = os.path.join(repo, 'cochabamba_data')  # Nuevo directorio para la data modificada\n",
    "output_file_path = os.path.join(output_dir, 'cochabamba_data.csv')  # Ruta completa al archivo CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f223f4fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La información modificada se ha guardado en el archivo D:\\trufiapp\\GANS\\cochabamba_data\\cochabamba_data.csv.\n",
      "Lista de IDs únicos de origin_id:\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]\n",
      "El máximo ID de destinos en Cochabamba es: 497\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Leer el archivo CSV original\n",
    "df = pd.read_csv(csv_file_path, parse_dates=['date_time'], encoding='latin1')\n",
    "\n",
    "# Filtrar registros que tienen 'Cochabamba' como ciudad de origen y destino\n",
    "cochabamba_records = df[\n",
    "    (df['origin_city'] == 'Cochabamba')\n",
    "].copy()\n",
    "\n",
    "# Crear columnas para índices H3 de origen y destino\n",
    "cochabamba_records['origin_h3_index'], cochabamba_records['origin_coords'] = zip(*cochabamba_records.apply(lambda row: get_h3_index_and_coords(row['origin_latitude'], row['origin_longitude']), axis=1))\n",
    "cochabamba_records['dest_h3_index'], cochabamba_records['dest_coords'] = zip(*cochabamba_records.apply(lambda row: get_h3_index_and_coords(row['destination_latitude'], row['destination_longitude']), axis=1))\n",
    "\n",
    "# Crear un diccionario que mapea cada índice H3 único a su propio ID único\n",
    "h3_index_to_id = {h3_index: idx + 1 for idx, h3_index in enumerate(pd.concat([cochabamba_records['origin_h3_index'], cochabamba_records['dest_h3_index']]).unique())}\n",
    "\n",
    "# Crear DataFrame con índices H3 únicos y sus IDs correspondientes y coordenadas\n",
    "h3_id_df = pd.DataFrame(list(h3_index_to_id.items()), columns=['h3_index', 'id'])\n",
    "h3_id_df[['latitude', 'longitude']] = pd.DataFrame(h3_id_df['h3_index'].apply(lambda h3_index: h3.h3_to_geo(h3_index)).to_list(), index=h3_id_df.index)\n",
    "\n",
    "# Guardar DataFrame de índices H3 y sus IDs en un archivo CSV separado\n",
    "h3_id_df.to_csv(csv_save_file_path, index=False)\n",
    "\n",
    "# Asignar los IDs únicos tanto para el origen como para el destino utilizando el diccionario\n",
    "cochabamba_records['origin_id'] = cochabamba_records['origin_h3_index'].map(h3_index_to_id)\n",
    "cochabamba_records['destination_id'] = cochabamba_records['dest_h3_index'].map(h3_index_to_id)\n",
    "\n",
    "# Crear un nuevo DataFrame solo con las columnas necesarias\n",
    "new_df = cochabamba_records[['date_time', 'hour', 'day_of_week', 'day_of_month', 'is_weekend', 'distance_meters', 'origin_id', 'destination_id']]\n",
    "\n",
    "# Guardar el nuevo DataFrame con IDs únicos y solo registros en Cochabamba en un nuevo directorio\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "new_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"La información modificada se ha guardado en el archivo {output_file_path}.\")\n",
    "max_origin_id = cochabamba_records['origin_id'].max()\n",
    "max_destination_id = cochabamba_records['destination_id'].max()\n",
    "\n",
    "\n",
    "# Obtener una lista de IDs únicos correspondientes a origin_id\n",
    "unique_origin_ids = cochabamba_records['origin_id'].unique().tolist()\n",
    "\n",
    "# Imprimir la lista de IDs únicos\n",
    "print(\"Lista de IDs únicos de origin_id:\")\n",
    "print(unique_origin_ids)\n",
    "\n",
    "print(f\"El máximo ID de destinos en Cochabamba es: {max_destination_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6ac5337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h3_index</th>\n",
       "      <th>id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>878b2c8a1ffffff</td>\n",
       "      <td>1</td>\n",
       "      <td>-17.397907</td>\n",
       "      <td>-66.169306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>878b2c8aeffffff</td>\n",
       "      <td>2</td>\n",
       "      <td>-17.375229</td>\n",
       "      <td>-66.167926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>878b2c8a3ffffff</td>\n",
       "      <td>3</td>\n",
       "      <td>-17.387033</td>\n",
       "      <td>-66.148877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>878b2c8a0ffffff</td>\n",
       "      <td>4</td>\n",
       "      <td>-17.409708</td>\n",
       "      <td>-66.150256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>878b2c98affffff</td>\n",
       "      <td>5</td>\n",
       "      <td>-17.455051</td>\n",
       "      <td>-66.153015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          h3_index  id   latitude  longitude\n",
       "0  878b2c8a1ffffff   1 -17.397907 -66.169306\n",
       "1  878b2c8aeffffff   2 -17.375229 -66.167926\n",
       "2  878b2c8a3ffffff   3 -17.387033 -66.148877\n",
       "3  878b2c8a0ffffff   4 -17.409708 -66.150256\n",
       "4  878b2c98affffff   5 -17.455051 -66.153015"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "repoCB = r'D:\\trufiapp\\GANS\\cochabamba_data'\n",
    "repo = r'D:\\trufiapp\\GANS'\n",
    "csv_file_path = os.path.join(repoCB, 'cochabamba_data.csv')\n",
    "csv_save_file_path = os.path.join(repo, 'id_index_h3.csv')\n",
    "\n",
    "# Load the DataFrame with H3 indices and coordinates\n",
    "h3_id_df = pd.read_csv(csv_save_file_path)\n",
    "\n",
    "# Calculate the average latitude and longitude\n",
    "avg_lat = h3_id_df['latitude'].mean()\n",
    "avg_lon = h3_id_df['longitude'].mean()\n",
    "\n",
    "# Create a map centered at the average coordinates\n",
    "m = folium.Map(location=[avg_lat, avg_lon], zoom_start=12)\n",
    "\n",
    "# Set to keep track of printed hexagons\n",
    "printed_hexagons = set()\n",
    "\n",
    "h3_id_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d1bf7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to add a polygon to the map with a label\n",
    "def add_polygon_with_label(m, hexagon, label):\n",
    "    folium.Polygon(\n",
    "        locations=hexagon,\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fill_color='blue',\n",
    "        fill_opacity=0.4,\n",
    "        tooltip=f'Label: {label}'  # Add tooltip with the label number\n",
    "    ).add_to(m)\n",
    "    printed_hexagons.add(label)  # Use 'label' as a unique identifier\n",
    "\n",
    "# Iterate over the records\n",
    "for idx, row in h3_id_df.iterrows():\n",
    "    # Get the coordinates and convert them to a tuple\n",
    "    coords = (row['latitude'], row['longitude'])\n",
    "    hexagon_coords = h3.h3_to_geo_boundary(row['h3_index'])  # Get hexagon coordinates\n",
    "\n",
    "    # Check if the hexagon has not been printed yet\n",
    "    if row['id'] not in printed_hexagons:  # Use 'id' as a unique identifier\n",
    "        add_polygon_with_label(m, hexagon_coords, row['id'])  # Use 'id' as a label\n",
    "\n",
    "# Save the map of origins and destinations in Cochabamba as an HTML file\n",
    "map_cochabamba_file_path = os.path.join(repo, 'h3_map_cochabamba_.html')\n",
    "m.save(map_cochabamba_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17b8e36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hexágonos impresos: 497\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Imprimir los hexágonos impresos\n",
    "print(\"Hexágonos impresos:\", len(printed_hexagons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f789ff5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# printed_hexagons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c149acc-38de-406c-8e90-ed2a90bf6378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\idaas\\\\anaconda3\\\\envs\\\\pyspark-env\\\\lib\\\\site-packages\\\\pyspark'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96516a08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-VOOKUKL:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importar las bibliotecas necesarias\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import col, lit, concat_ws, collect_set\n",
    "from pyspark.sql.types import TimestampType, StructField, StringType, IntegerType, StructType\n",
    "from pyspark.sql.functions import date_format\n",
    "sc = SparkContext.getOrCreate()\n",
    "# Primera sesión de Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Trufi\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"3g\") \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .config(\"spark.master\", \"local\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Configurar el número de particiones\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f278927f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------------+---------+-----------+\n",
      "|      UniqueColumn|Pickup_datetime_hourly|origin_id|Trips_count|\n",
      "+------------------+----------------------+---------+-----------+\n",
      "|2022-09-12 09:00_1|      2022-09-12 09:00|        1|          1|\n",
      "|2022-09-12 11:00_3|      2022-09-12 11:00|        3|          1|\n",
      "|2022-09-12 19:00_5|      2022-09-12 19:00|        5|          3|\n",
      "|2022-09-13 17:00_9|      2022-09-13 17:00|        9|          2|\n",
      "|2022-09-13 18:00_3|      2022-09-13 18:00|        3|          7|\n",
      "+------------------+----------------------+---------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------------------+----------------------+---------+-----------+\n",
      "|       UniqueColumn|Pickup_datetime_hourly|origin_id|Trips_count|\n",
      "+-------------------+----------------------+---------+-----------+\n",
      "| 2023-12-26 22:00_7|      2023-12-26 22:00|        7|          1|\n",
      "| 2023-12-26 22:00_3|      2023-12-26 22:00|        3|         15|\n",
      "|2023-12-26 22:00_15|      2023-12-26 22:00|       15|          2|\n",
      "| 2023-12-26 22:00_9|      2023-12-26 22:00|        9|          2|\n",
      "|2023-12-26 22:00_23|      2023-12-26 22:00|       23|          1|\n",
      "+-------------------+----------------------+---------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Total rows in the aggregated DataFrame: 134553\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the path to your CSV file\n",
    "csv_file_path = \"D:\\\\trufiapp\\\\GANS\\\\cochabamba_data\\\\cochabamba_data.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Convert the timestamp to an hourly format\n",
    "df = df.withColumn(\"Pickup_datetime_hourly\", date_format(col(\"date_time\").cast(\"timestamp\"), \"yyyy-MM-dd HH:00\"))\n",
    "\n",
    "# Create a trip count column\n",
    "df = df.withColumn(\"Trip_count\", lit(1))\n",
    "\n",
    "# Group by pickup datetime, location ID, and aggregate the trip count\n",
    "hourly_aggregated = df.groupby(['Pickup_datetime_hourly', 'origin_id']).agg({'Trip_count': 'count'}).withColumnRenamed(\"count(Trip_count)\", \"Trips_count\")\n",
    "\n",
    "# Crear la columna única usando concat_ws\n",
    "hourly_aggregated = hourly_aggregated.select(\n",
    "    concat_ws('_', col(\"Pickup_datetime_hourly\"), col(\"origin_id\")).alias(\"UniqueColumn\"),\n",
    "    col(\"Pickup_datetime_hourly\"),\n",
    "    col(\"origin_id\"),\n",
    "    col(\"Trips_count\")\n",
    ")\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "hourly_aggregated.show(5)\n",
    "hourly_aggregated.orderBy(col(\"Pickup_datetime_hourly\").desc()).show(5)\n",
    "# Count the number of rows in the aggregated DataFrame\n",
    "print(\"Total rows in the aggregated DataFrame:\", hourly_aggregated.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7cd8f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+-----------+------------+----------+---------------+---------+--------------+\n",
      "|          date_time|hour|day_of_week|day_of_month|is_weekend|distance_meters|origin_id|destination_id|\n",
      "+-------------------+----+-----------+------------+----------+---------------+---------+--------------+\n",
      "|2022-09-12 09:55:24|   9|          0|          12|         0|            934|        1|             2|\n",
      "|2022-09-12 09:55:32|   9|          0|          12|         0|            998|        2|             3|\n",
      "|2022-09-12 10:05:28|  10|          0|          12|         0|            998|        3|             2|\n",
      "|2022-09-12 10:07:36|  10|          0|          12|         0|            797|        2|             3|\n",
      "|2022-09-12 11:27:27|  11|          0|          12|         0|            797|        3|             2|\n",
      "|2022-09-12 11:27:34|  11|          0|          12|         0|            797|        2|             3|\n",
      "|2022-09-12 18:26:30|  18|          0|          12|         0|            959|        4|             4|\n",
      "|2022-09-12 18:54:27|  18|          0|          12|         0|           1938|        3|            19|\n",
      "|2022-09-12 18:54:42|  18|          0|          12|         0|           1938|        3|            19|\n",
      "|2022-09-12 19:12:20|  19|          0|          12|         0|           3255|        5|             2|\n",
      "|2022-09-12 19:13:24|  19|          0|          12|         0|           3255|        5|             2|\n",
      "|2022-09-12 19:13:29|  19|          0|          12|         0|           3255|        5|             2|\n",
      "|2022-09-12 23:08:33|  23|          0|          12|         0|           5449|        6|             3|\n",
      "|2022-09-12 23:34:57|  23|          0|          12|         0|            797|        2|             3|\n",
      "|2022-09-12 23:35:39|  23|          0|          12|         0|           5449|        6|             3|\n",
      "|2022-09-12 23:35:47|  23|          0|          12|         0|           6236|        6|             2|\n",
      "|2022-09-12 23:37:09|  23|          0|          12|         0|           6236|        6|             2|\n",
      "|2022-09-13 09:02:39|   9|          1|          13|         0|            797|        3|             2|\n",
      "|2022-09-13 17:37:34|  17|          1|          13|         0|           2363|        3|             9|\n",
      "|2022-09-13 17:49:00|  17|          1|          13|         0|           3242|        4|             9|\n",
      "+-------------------+----+-----------+------------+----------+---------------+---------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tu DataFrame original\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26e09c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"OriginLocationID\", col(\"origin_id\").cast(\"string\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea1575b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+----------------+---------------+\n",
      "|        UniqueColumn|          date_time|OriginLocationID|distance_meters|\n",
      "+--------------------+-------------------+----------------+---------------+\n",
      "|2022-09-12 09:55:...|2022-09-12 09:55:24|               1|            934|\n",
      "|2022-09-12 09:55:...|2022-09-12 09:55:32|               2|            998|\n",
      "|2022-09-12 10:05:...|2022-09-12 10:05:28|               3|            998|\n",
      "|2022-09-12 10:07:...|2022-09-12 10:07:36|               2|            797|\n",
      "|2022-09-12 11:27:...|2022-09-12 11:27:27|               3|            797|\n",
      "|2022-09-12 11:27:...|2022-09-12 11:27:34|               2|            797|\n",
      "|2022-09-12 18:26:...|2022-09-12 18:26:30|               4|            959|\n",
      "|2022-09-12 18:54:...|2022-09-12 18:54:27|               3|           1938|\n",
      "|2022-09-12 18:54:...|2022-09-12 18:54:42|               3|           1938|\n",
      "|2022-09-12 19:12:...|2022-09-12 19:12:20|               5|           3255|\n",
      "|2022-09-12 19:13:...|2022-09-12 19:13:24|               5|           3255|\n",
      "|2022-09-12 19:13:...|2022-09-12 19:13:29|               5|           3255|\n",
      "|2022-09-12 23:08:...|2022-09-12 23:08:33|               6|           5449|\n",
      "|2022-09-12 23:34:...|2022-09-12 23:34:57|               2|            797|\n",
      "|2022-09-12 23:35:...|2022-09-12 23:35:39|               6|           5449|\n",
      "|2022-09-12 23:35:...|2022-09-12 23:35:47|               6|           6236|\n",
      "|2022-09-12 23:37:...|2022-09-12 23:37:09|               6|           6236|\n",
      "|2022-09-13 09:02:...|2022-09-13 09:02:39|               3|            797|\n",
      "|2022-09-13 17:37:...|2022-09-13 17:37:34|               3|           2363|\n",
      "|2022-09-13 17:49:...|2022-09-13 17:49:00|               4|           3242|\n",
      "+--------------------+-------------------+----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1361823"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reemplaza \"df\" con tu DataFrame actual y \"origin_id_string\" con el nombre de tu nueva columna\n",
    "df = df.withColumn(\"UniqueColumn\", concat_ws('_', df[\"date_time\"], df[\"OriginLocationID\"]))\n",
    "\n",
    "# Seleccionar las columnas deseadas\n",
    "df = df.select(\"UniqueColumn\", \"date_time\", \"OriginLocationID\",\"distance_meters\")\n",
    "\n",
    "# Mostrar algunas filas para verificar\n",
    "df.show()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f0b6d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de filas después de filtrar: 1212411\n",
      "Número de columnas: 4\n"
     ]
    }
   ],
   "source": [
    "# Assuming df is your DataFrame\n",
    "df = df[df['distance_meters'] >= 500]\n",
    "\n",
    "# Obtener el número de filas y columnas después de aplicar el filtro\n",
    "num_filas = df.count()\n",
    "num_columnas = len(df.columns)\n",
    "\n",
    "# Imprimir el número de filas y columnas del DataFrame filtrado\n",
    "print(\"Número de filas después de filtrar:\", num_filas)\n",
    "print(\"Número de columnas:\", num_columnas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5202653c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11300</th>\n",
       "      <td>2023-12-26 20:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11301</th>\n",
       "      <td>2023-12-26 21:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11302</th>\n",
       "      <td>2023-12-26 22:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11303</th>\n",
       "      <td>2023-12-26 23:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11304</th>\n",
       "      <td>2023-12-27 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                timestamp\n",
       "11300 2023-12-26 20:00:00\n",
       "11301 2023-12-26 21:00:00\n",
       "11302 2023-12-26 22:00:00\n",
       "11303 2023-12-26 23:00:00\n",
       "11304 2023-12-27 00:00:00"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def generate_series(start, stop, interval):\n",
    "    start = pd.Timestamp(start)\n",
    "    stop = pd.Timestamp(stop)\n",
    "    timestamps = pd.date_range(start, stop, freq=f'{interval}s')\n",
    "    return pd.DataFrame({'timestamp': timestamps})\n",
    "\n",
    "# Generar el DataFrame de marcas de tiempo\n",
    "timestamp_df = generate_series(\"2022-09-12\", \"2023-12-27\", 3600)  # Intervalo de 1 hora\n",
    "\n",
    "timestamp_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f64a0e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11304"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generar el DataFrame de marcas de tiempo con intervalo de 1 hora\n",
    "timestamp_df = pd.date_range(\"2022-09-12\", \"2023-12-27\", freq='1H').to_frame(name='timestamp')\n",
    "\n",
    "# Aplicar el formato de fecha deseado\n",
    "timestamp_df['hourly_timestamp'] = timestamp_df['timestamp'].dt.strftime('%Y-%m-%d %H:00')\n",
    "# Eliminar la última fila del DataFrame\n",
    "timestamp_df = timestamp_df.iloc[:-1]\n",
    "\n",
    "# Verificar la longitud después de la eliminación\n",
    "len(timestamp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "087804f7-09e9-4873-94f0-ca3fda640280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>hourly_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-09-12 00:00:00</th>\n",
       "      <td>2022-09-12 00:00:00</td>\n",
       "      <td>2022-09-12 00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-12 01:00:00</th>\n",
       "      <td>2022-09-12 01:00:00</td>\n",
       "      <td>2022-09-12 01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-12 02:00:00</th>\n",
       "      <td>2022-09-12 02:00:00</td>\n",
       "      <td>2022-09-12 02:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-12 03:00:00</th>\n",
       "      <td>2022-09-12 03:00:00</td>\n",
       "      <td>2022-09-12 03:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-12 04:00:00</th>\n",
       "      <td>2022-09-12 04:00:00</td>\n",
       "      <td>2022-09-12 04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-26 19:00:00</th>\n",
       "      <td>2023-12-26 19:00:00</td>\n",
       "      <td>2023-12-26 19:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-26 20:00:00</th>\n",
       "      <td>2023-12-26 20:00:00</td>\n",
       "      <td>2023-12-26 20:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-26 21:00:00</th>\n",
       "      <td>2023-12-26 21:00:00</td>\n",
       "      <td>2023-12-26 21:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-26 22:00:00</th>\n",
       "      <td>2023-12-26 22:00:00</td>\n",
       "      <td>2023-12-26 22:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-26 23:00:00</th>\n",
       "      <td>2023-12-26 23:00:00</td>\n",
       "      <td>2023-12-26 23:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11304 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              timestamp  hourly_timestamp\n",
       "2022-09-12 00:00:00 2022-09-12 00:00:00  2022-09-12 00:00\n",
       "2022-09-12 01:00:00 2022-09-12 01:00:00  2022-09-12 01:00\n",
       "2022-09-12 02:00:00 2022-09-12 02:00:00  2022-09-12 02:00\n",
       "2022-09-12 03:00:00 2022-09-12 03:00:00  2022-09-12 03:00\n",
       "2022-09-12 04:00:00 2022-09-12 04:00:00  2022-09-12 04:00\n",
       "...                                 ...               ...\n",
       "2023-12-26 19:00:00 2023-12-26 19:00:00  2023-12-26 19:00\n",
       "2023-12-26 20:00:00 2023-12-26 20:00:00  2023-12-26 20:00\n",
       "2023-12-26 21:00:00 2023-12-26 21:00:00  2023-12-26 21:00\n",
       "2023-12-26 22:00:00 2023-12-26 22:00:00  2023-12-26 22:00\n",
       "2023-12-26 23:00:00 2023-12-26 23:00:00  2023-12-26 23:00\n",
       "\n",
       "[11304 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1cc9299e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extrae los IDs de ubicación del conjunto de datos y guárdalos en una lista\n",
    "# Pick_up_LocationID = df.select(collect_set('OriginLocationID').alias('locationID_list')).first()['locationID_list']\n",
    "# # Convertir la lista de strings a números\n",
    "# Pick_up_LocationID_numeric = list(map(int, Pick_up_LocationID))\n",
    "\n",
    "# Pick_up_LocationID.sort()\n",
    "\n",
    "# # Verificar el resultado\n",
    "# print(Pick_up_LocationID_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3459fa3f-72f5-4630-9ea1-673e98aba53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de filas en Pick_up_LocationID: 59\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Define la ruta al archivo id_index_h3.csv\n",
    "id_index_file_path = \"D:\\\\Trufiapp\\\\GANS\\\\id_index_h3.csv\"\n",
    "\n",
    "# Lee el archivo en un DataFrame de Spark\n",
    "id_index_df = spark.read.csv(id_index_file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Encuentra los IDs de ubicación únicos del DataFrame original\n",
    "Pick_up_LocationID = id_index_df.select('id').distinct()\n",
    "Pick_up_LocationID = Pick_up_LocationID.orderBy(\"id\")\n",
    "Pick_up_LocationID = Pick_up_LocationID.withColumnRenamed(\"id\", \"OriginLocationID\")\n",
    "# Convierte la columna 'OriginLocationID' a tipo entero\n",
    "Pick_up_LocationID = Pick_up_LocationID.withColumn(\"OriginLocationID\", Pick_up_LocationID[\"OriginLocationID\"].cast(IntegerType()))\n",
    "\n",
    "# Convertir elementos de unique_origin_ids a enteros\n",
    "unique_origin_ids_int = [int(id) for id in unique_origin_ids]\n",
    "\n",
    "# Crear un DataFrame de Spark a partir de la lista unique_origin_ids_int\n",
    "row_list = [Row(OriginLocationID=id) for id in unique_origin_ids_int]\n",
    "schema = [\"OriginLocationID\"]\n",
    "Pick_up_LocationID = spark.createDataFrame(row_list, schema=schema)\n",
    "\n",
    "# Mostrar la cantidad de filas en el nuevo DataFrame\n",
    "print(\"Cantidad de filas en Pick_up_LocationID:\", Pick_up_LocationID.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2753b044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define una función para agregar la columna locationID al DataFrame de timestamp\n",
    "def locations_timestamp_generator(dataframe, location_id):\n",
    "    dataframe[\"OriginLocationID\"] = location_id\n",
    "    return dataframe\n",
    "\n",
    "# Define el esquema para el DataFrame final\n",
    "data_schema = [StructField(\"timestamp\", TimestampType(), True),\n",
    "               StructField(\"hourly_timestamp\", StringType(), True),\n",
    "               StructField(\"OriginLocationID\", IntegerType(), True)]\n",
    "final_struct = StructType(fields=data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c6e0dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea un DataFrame vacío con las columnas requeridas\n",
    "data_schema = [\"timestamp\", \"hourly_timestamp\", \"OriginLocationID\"]\n",
    "ID_plus_timestamp = pd.DataFrame(columns=data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dcc9dcb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp           666936\n",
       "hourly_timestamp    666936\n",
       "OriginLocationID    666936\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lista para almacenar DataFrames temporales\n",
    "frames = []\n",
    "\n",
    "# Itera sobre los IDs de ubicación y agrega las filas al DataFrame final\n",
    "for row in Pick_up_LocationID.collect():\n",
    "    location_id = row[\"OriginLocationID\"]\n",
    "    timestamp_df_copy = timestamp_df.copy()\n",
    "    timestamp_df_copy = locations_timestamp_generator(timestamp_df_copy, location_id)\n",
    "    frames.append(timestamp_df_copy)\n",
    "\n",
    "# Concatena todos los DataFrames en la lista\n",
    "ID_plus_timestamp = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "# Muestra el resultado\n",
    "ID_plus_timestamp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb3364c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de filas en el DataFrame resultante: 666936\n",
      "                 timestamp  hourly_timestamp  OriginLocationID\n",
      "666931 2023-12-26 19:00:00  2023-12-26 19:00                59\n",
      "666932 2023-12-26 20:00:00  2023-12-26 20:00                59\n",
      "666933 2023-12-26 21:00:00  2023-12-26 21:00                59\n",
      "666934 2023-12-26 22:00:00  2023-12-26 22:00                59\n",
      "666935 2023-12-26 23:00:00  2023-12-26 23:00                59\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Total de filas en el DataFrame resultante:\", len(ID_plus_timestamp))\n",
    "print(ID_plus_timestamp.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7245109f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extraer los locationID únicos de ID_plus_timestamp\n",
    "# location_ID_list2 = ID_plus_timestamp[\"OriginLocationID\"].unique()\n",
    "\n",
    "# # Verificar si hay algún locationID en ID_plus_timestamp que no exista en Pick_up_LocationID\n",
    "# unique_list = [item for item in location_ID_list2 if item not in Pick_up_LocationID]\n",
    "\n",
    "# # Debería estar vacío\n",
    "# print(\"Lista de locationID únicos no encontrados en Pick_up_LocationID:\", unique_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "197304de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       UniqueColumn_G           timestamp  hourly_timestamp  OriginLocationID\n",
      "0  2022-09-12 00:00_1 2022-09-12 00:00:00  2022-09-12 00:00                 1\n",
      "1  2022-09-12 01:00_1 2022-09-12 01:00:00  2022-09-12 01:00                 1\n",
      "2  2022-09-12 02:00_1 2022-09-12 02:00:00  2022-09-12 02:00                 1\n",
      "3  2022-09-12 03:00_1 2022-09-12 03:00:00  2022-09-12 03:00                 1\n",
      "4  2022-09-12 04:00_1 2022-09-12 04:00:00  2022-09-12 04:00                 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UniqueColumn_G      666936\n",
       "timestamp           666936\n",
       "hourly_timestamp    666936\n",
       "OriginLocationID    666936\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cambiar el locationID de entero a cadena\n",
    "ID_plus_timestamp[\"locationID_string\"] = ID_plus_timestamp[\"OriginLocationID\"].astype(str)\n",
    "\n",
    "# Crear la columna única utilizando concat_ws\n",
    "ID_plus_timestamp[\"UniqueColumn_G\"] = ID_plus_timestamp[\"hourly_timestamp\"] + \"_\" + ID_plus_timestamp[\"locationID_string\"]\n",
    "\n",
    "# Seleccionar las columnas relevantes\n",
    "ID_plus_timestamp = ID_plus_timestamp[[\"UniqueColumn_G\", \"timestamp\", \"hourly_timestamp\", \"OriginLocationID\"]]\n",
    "\n",
    "# Mostrar algunas filas para verificar\n",
    "print(ID_plus_timestamp.head())\n",
    "ID_plus_timestamp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d396b61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_plus_timestamp = spark.createDataFrame(ID_plus_timestamp.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b9ebd43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(UniqueColumn='2022-09-12 09:00_1', Pickup_datetime_hourly='2022-09-12 09:00', origin_id=1, Trips_count=1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_aggregated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ade4a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, concat_ws\n",
    "# Cambiar el ID de ubicación de entero a cadena\n",
    "id_plus_timestamp = id_plus_timestamp.withColumn(\"locationID_string\", col(\"OriginLocationID\").cast(\"int\"))\n",
    "\n",
    "# Crear la columna única usando concat_ws\n",
    "id_plus_timestamp = id_plus_timestamp.select(\n",
    "    concat_ws('_', col(\"hourly_timestamp\"), col(\"locationID_string\")).alias(\"UniqueColumn_G\"),\n",
    "    col(\"timestamp\"),\n",
    "    col(\"hourly_timestamp\"),\n",
    "    col(\"OriginLocationID\"),\n",
    "    col(\"hourly_timestamp\")\n",
    ")\n",
    "\n",
    "# Realizar la unión de ambos DataFrames usando la columna única\n",
    "pick_aggregated = hourly_aggregated.join(id_plus_timestamp, hourly_aggregated[\"UniqueColumn\"] == id_plus_timestamp[\"UniqueColumn_G\"], \"right\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "283b9e00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(UniqueColumn=None, Pickup_datetime_hourly=None, origin_id=None, Trips_count=None, UniqueColumn_G='2022-09-12 00:00_1', timestamp=datetime.datetime(2022, 9, 12, 0, 0), hourly_timestamp='2022-09-12 00:00', OriginLocationID=1, hourly_timestamp='2022-09-12 00:00')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostrar algunas filas del resultado\n",
    "# pick_aggregated.count()\n",
    "pick_aggregated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "404c3a6d-dc2a-4b06-b68c-289fccff6801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------+----------------+--------------------+-------------------+\n",
      "|          timestamp|hourly_timestamp|OriginLocationID|origin_request_count|     UniqueColumn_G|\n",
      "+-------------------+----------------+----------------+--------------------+-------------------+\n",
      "|2022-09-12 00:00:00|2022-09-12 00:00|               1|                   0| 2022-09-12 00:00_1|\n",
      "|2022-09-12 00:00:00|2022-09-12 00:00|               2|                   0| 2022-09-12 00:00_2|\n",
      "|2022-09-12 00:00:00|2022-09-12 00:00|               3|                   0| 2022-09-12 00:00_3|\n",
      "|2022-09-12 00:00:00|2022-09-12 00:00|               4|                   0| 2022-09-12 00:00_4|\n",
      "|2022-09-12 00:00:00|2022-09-12 00:00|               5|                   0| 2022-09-12 00:00_5|\n",
      "|2022-09-12 00:00:00|2022-09-12 00:00|               6|                   0| 2022-09-12 00:00_6|\n",
      "|2022-09-12 00:00:00|2022-09-12 00:00|               7|                   0| 2022-09-12 00:00_7|\n",
      "|2022-09-12 00:00:00|2022-09-12 00:00|               8|                   0| 2022-09-12 00:00_8|\n",
      "|2022-09-12 00:00:00|2022-09-12 00:00|               9|                   0| 2022-09-12 00:00_9|\n",
      "|2022-09-12 00:00:00|2022-09-12 00:00|              10|                   0|2022-09-12 00:00_10|\n",
      "|2022-09-12 00:00:00|2022-09-12 00:00|              11|                   0|2022-09-12 00:00_11|\n",
      "|2022-09-12 00:00:00|2022-09-12 00:00|              12|                   0|2022-09-12 00:00_12|\n",
      "|2022-09-12 00:00:00|2022-09-12 00:00|              13|                   0|2022-09-12 00:00_13|\n",
      "|2022-09-12 00:00:00|2022-09-12 00:00|              14|                   0|2022-09-12 00:00_14|\n",
      "|2022-09-12 00:00:00|2022-09-12 00:00|              15|                   0|2022-09-12 00:00_15|\n",
      "|2022-09-12 00:00:00|2022-09-12 00:00|              16|                   0|2022-09-12 00:00_16|\n",
      "|2022-09-12 00:00:00|2022-09-12 00:00|              17|                   0|2022-09-12 00:00_17|\n",
      "|2022-09-12 00:00:00|2022-09-12 00:00|              18|                   0|2022-09-12 00:00_18|\n",
      "|2022-09-12 00:00:00|2022-09-12 00:00|              19|                   0|2022-09-12 00:00_19|\n",
      "|2022-09-12 00:00:00|2022-09-12 00:00|              20|                   0|2022-09-12 00:00_20|\n",
      "+-------------------+----------------+----------------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Seleccionar solo las columnas deseadas\n",
    "columnas_deseadas = ['timestamp', 'hourly_timestamp', 'OriginLocationID', 'Trips_count', 'UniqueColumn_G']\n",
    "pick_aggregated_o = pick_aggregated.select(*columnas_deseadas)\n",
    "\n",
    "# Ordenar por las columnas en el orden deseado\n",
    "pick_aggregated_o = pick_aggregated_o.orderBy(*[col(column) for column in columnas_deseadas])\n",
    "# Reemplazar los valores nulos en las columnas especificadas\n",
    "pick_aggregated_o = pick_aggregated_o.na.fill(0, \"Trips_count\")\n",
    "pick_aggregated_o = pick_aggregated_o.na.fill(0, \"OriginLocationID\")\n",
    "\n",
    "# Cambiar el nombre de Trips_count a origin_request_count\n",
    "pick_aggregated_o = pick_aggregated_o.withColumnRenamed(\"Trips_count\", \"origin_request_count\")\n",
    "\n",
    "# Mostrar el DataFrame ordenado\n",
    "pick_aggregated_o.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e0b354d6-337f-44e7-bacc-668ee0107c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "repo = r'D:\\trufiapp\\GANS'\n",
    "out_agg_archivo_csv = os.path.join(repo, 'out_agg_archivo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c68234f0-f15b-4d1f-8d7b-0791d3538125",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Especifica la ruta del archivo CSV en el sistema de archivos local\n",
    "csv_local_path = \"file:///\" + out_agg_archivo_csv\n",
    "\n",
    "# Guardar el DataFrame en formato CSV en el sistema de archivos local\n",
    "pick_aggregated_o.write.mode(\"overwrite\").option(\"header\", \"true\").csv(csv_local_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2cee0b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- hourly_timestamp: timestamp (nullable = true)\n",
      " |-- OriginLocationID: integer (nullable = true)\n",
      " |-- origin_request_count: integer (nullable = true)\n",
      " |-- UniqueColumn_G: string (nullable = true)\n",
      "\n",
      "+-------------------+-------------------+----------------+--------------------+-------------------+\n",
      "|          timestamp|   hourly_timestamp|OriginLocationID|origin_request_count|     UniqueColumn_G|\n",
      "+-------------------+-------------------+----------------+--------------------+-------------------+\n",
      "|2023-01-05 08:00:00|2023-01-05 08:00:00|              32|                   0|2023-01-05 08:00_32|\n",
      "|2023-01-05 08:00:00|2023-01-05 08:00:00|              33|                   0|2023-01-05 08:00_33|\n",
      "|2023-01-05 08:00:00|2023-01-05 08:00:00|              34|                   0|2023-01-05 08:00_34|\n",
      "|2023-01-05 08:00:00|2023-01-05 08:00:00|              35|                   0|2023-01-05 08:00_35|\n",
      "|2023-01-05 08:00:00|2023-01-05 08:00:00|              36|                   0|2023-01-05 08:00_36|\n",
      "|2023-01-05 08:00:00|2023-01-05 08:00:00|              37|                   0|2023-01-05 08:00_37|\n",
      "|2023-01-05 08:00:00|2023-01-05 08:00:00|              38|                   0|2023-01-05 08:00_38|\n",
      "|2023-01-05 08:00:00|2023-01-05 08:00:00|              39|                   0|2023-01-05 08:00_39|\n",
      "|2023-01-05 08:00:00|2023-01-05 08:00:00|              40|                   0|2023-01-05 08:00_40|\n",
      "|2023-01-05 08:00:00|2023-01-05 08:00:00|              41|                   0|2023-01-05 08:00_41|\n",
      "|2023-01-05 08:00:00|2023-01-05 08:00:00|              42|                   0|2023-01-05 08:00_42|\n",
      "|2023-01-05 08:00:00|2023-01-05 08:00:00|              43|                   0|2023-01-05 08:00_43|\n",
      "|2023-01-05 08:00:00|2023-01-05 08:00:00|              44|                   0|2023-01-05 08:00_44|\n",
      "|2023-01-05 08:00:00|2023-01-05 08:00:00|              45|                   0|2023-01-05 08:00_45|\n",
      "|2023-01-05 08:00:00|2023-01-05 08:00:00|              46|                   0|2023-01-05 08:00_46|\n",
      "|2023-01-05 08:00:00|2023-01-05 08:00:00|              47|                   0|2023-01-05 08:00_47|\n",
      "|2023-01-05 08:00:00|2023-01-05 08:00:00|              48|                   0|2023-01-05 08:00_48|\n",
      "|2023-01-05 08:00:00|2023-01-05 08:00:00|              49|                   0|2023-01-05 08:00_49|\n",
      "|2023-01-05 08:00:00|2023-01-05 08:00:00|              50|                   0|2023-01-05 08:00_50|\n",
      "|2023-01-05 08:00:00|2023-01-05 08:00:00|              51|                   0|2023-01-05 08:00_51|\n",
      "+-------------------+-------------------+----------------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lee el archivo CSV en un DataFrame\n",
    "df = spark.read.csv(\"file:///\" + out_agg_archivo_csv, header=True, inferSchema=True)\n",
    "\n",
    "# Muestra el esquema del DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# Muestra las primeras filas del DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0f71df59",
   "metadata": {},
   "outputs": [],
   "source": [
    "##AHORA PARA DESTINATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63b193b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4397d754-cd61-47b4-9b6c-a17e300784c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------------+--------------+-----------+\n",
      "|       UniqueColumn|Pickup_datetime_hourly|destination_id|Trips_count|\n",
      "+-------------------+----------------------+--------------+-----------+\n",
      "| 2022-09-12 09:00_3|      2022-09-12 09:00|             3|          1|\n",
      "| 2022-09-12 11:00_3|      2022-09-12 11:00|             3|          1|\n",
      "| 2022-09-13 17:00_9|      2022-09-13 17:00|             9|          3|\n",
      "|2022-09-13 18:00_61|      2022-09-13 18:00|            61|          1|\n",
      "|2022-09-13 18:00_13|      2022-09-13 18:00|            13|          1|\n",
      "| 2022-09-13 18:00_3|      2022-09-13 18:00|             3|          4|\n",
      "|2022-09-13 18:00_12|      2022-09-13 18:00|            12|          1|\n",
      "| 2022-09-13 19:00_9|      2022-09-13 19:00|             9|          2|\n",
      "| 2022-09-13 19:00_2|      2022-09-13 19:00|             2|          1|\n",
      "|2022-09-13 20:00_63|      2022-09-13 20:00|            63|          2|\n",
      "| 2022-09-13 21:00_4|      2022-09-13 21:00|             4|          3|\n",
      "| 2022-09-13 22:00_1|      2022-09-13 22:00|             1|          1|\n",
      "| 2022-09-14 01:00_8|      2022-09-14 01:00|             8|          1|\n",
      "| 2022-09-14 07:00_3|      2022-09-14 07:00|             3|          1|\n",
      "| 2022-09-14 07:00_5|      2022-09-14 07:00|             5|          1|\n",
      "| 2022-09-14 08:00_4|      2022-09-14 08:00|             4|          3|\n",
      "|2022-09-14 08:00_62|      2022-09-14 08:00|            62|          1|\n",
      "| 2022-09-14 09:00_7|      2022-09-14 09:00|             7|          2|\n",
      "|2022-09-14 09:00_15|      2022-09-14 09:00|            15|          1|\n",
      "|2022-09-14 09:00_71|      2022-09-14 09:00|            71|          1|\n",
      "+-------------------+----------------------+--------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Total rows in the aggregated DataFrame: 222976\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the path to your CSV file\n",
    "csv_file_path = \"D:\\\\trufiapp\\\\GANS\\\\cochabamba_data\\\\cochabamba_data.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Convert the timestamp to an hourly format\n",
    "df = df.withColumn(\"Pickup_datetime_hourly\", date_format(col(\"date_time\").cast(\"timestamp\"), \"yyyy-MM-dd HH:00\"))\n",
    "\n",
    "# Create a trip count column\n",
    "df = df.withColumn(\"Trip_count\", lit(1))\n",
    "\n",
    "# Group by pickup datetime, location ID, and aggregate the trip count\n",
    "hourly_aggregated = df.groupby(['Pickup_datetime_hourly', 'destination_id']).agg({'Trip_count': 'count'}).withColumnRenamed(\"count(Trip_count)\", \"Trips_count\")\n",
    "\n",
    "# Crear la columna única usando concat_ws\n",
    "hourly_aggregated = hourly_aggregated.select(\n",
    "    concat_ws('_', col(\"Pickup_datetime_hourly\"), col(\"destination_id\")).alias(\"UniqueColumn\"),\n",
    "    col(\"Pickup_datetime_hourly\"),\n",
    "    col(\"destination_id\"),\n",
    "    col(\"Trips_count\")\n",
    ")\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "hourly_aggregated.show()\n",
    "\n",
    "# Count the number of rows in the aggregated DataFrame\n",
    "print(\"Total rows in the aggregated DataFrame:\", hourly_aggregated.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2be1799d-05af-4e50-be42-8991713bce80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------+----------------+-------------------------+--------------------+\n",
      "|     UniqueColumn_G|hourly_timestamp|OriginLocationID|destination_request_count|origin_request_count|\n",
      "+-------------------+----------------+----------------+-------------------------+--------------------+\n",
      "| 2022-09-12 00:00_1|2022-09-12 00:00|               1|                     null|                   0|\n",
      "| 2022-09-12 03:00_1|2022-09-12 03:00|               1|                     null|                   0|\n",
      "| 2022-09-12 08:00_1|2022-09-12 08:00|               1|                     null|                   0|\n",
      "| 2022-09-12 10:00_1|2022-09-12 10:00|               1|                     null|                   0|\n",
      "| 2022-09-12 11:00_1|2022-09-12 11:00|               1|                     null|                   0|\n",
      "| 2022-09-12 12:00_1|2022-09-12 12:00|               1|                     null|                   0|\n",
      "| 2022-09-12 13:00_1|2022-09-12 13:00|               1|                     null|                   0|\n",
      "| 2022-09-12 15:00_1|2022-09-12 15:00|               1|                     null|                   0|\n",
      "| 2022-09-12 17:00_1|2022-09-12 17:00|               1|                     null|                   0|\n",
      "|2023-05-20 12:00_30|2023-05-20 12:00|              30|                     null|                   0|\n",
      "|2023-05-20 23:00_30|2023-05-20 23:00|              30|                     null|                   0|\n",
      "|2023-09-07 17:00_15|2023-09-07 17:00|              15|                        2|                   2|\n",
      "|2023-09-07 18:00_15|2023-09-07 18:00|              15|                        1|                   3|\n",
      "|2023-09-07 19:00_15|2023-09-07 19:00|              15|                        2|                   2|\n",
      "|2023-09-07 22:00_15|2023-09-07 22:00|              15|                     null|                   0|\n",
      "|2023-09-08 00:00_15|2023-09-08 00:00|              15|                     null|                   0|\n",
      "|2023-09-08 03:00_15|2023-09-08 03:00|              15|                     null|                   0|\n",
      "|2023-09-08 05:00_15|2023-09-08 05:00|              15|                     null|                   1|\n",
      "|2023-09-08 08:00_15|2023-09-08 08:00|              15|                        4|                   7|\n",
      "|2023-09-08 09:00_15|2023-09-08 09:00|              15|                        5|                  10|\n",
      "+-------------------+----------------+----------------+-------------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "666936"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Realiza un left join de los DataFrames usando las columnas UniqueColumn y UniqueColumn_G\n",
    "merged_df = pick_aggregated_o.join(hourly_aggregated, pick_aggregated_o[\"UniqueColumn_G\"] == hourly_aggregated[\"UniqueColumn\"], \"left_outer\")\n",
    "\n",
    "# Selecciona las columnas deseadas en el DataFrame final\n",
    "final_merged = merged_df.select(\n",
    "    pick_aggregated_o[\"UniqueColumn_G\"],\n",
    "    pick_aggregated_o[\"hourly_timestamp\"],\n",
    "    pick_aggregated_o[\"OriginLocationID\"],\n",
    "    hourly_aggregated[\"Trips_count\"].alias(\"destination_request_count\"),\n",
    "    pick_aggregated_o[\"origin_request_count\"]\n",
    ")\n",
    "\n",
    "# Muestra el DataFrame final\n",
    "final_merged.show()\n",
    "final_merged.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a72c2605-0a18-44ca-a5e8-1d020f136a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merged = final_merged.na.fill(0, \"destination_request_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e3c37565-5283-4957-aeb3-bae6e933cb3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------+----------------+-------------------------+--------------------+\n",
      "|     UniqueColumn_G|hourly_timestamp|OriginLocationID|destination_request_count|origin_request_count|\n",
      "+-------------------+----------------+----------------+-------------------------+--------------------+\n",
      "|2023-12-26 23:00_15|2023-12-26 23:00|              15|                        0|                   0|\n",
      "|2023-12-26 23:00_13|2023-12-26 23:00|              13|                        0|                   0|\n",
      "|2023-12-26 23:00_14|2023-12-26 23:00|              14|                        0|                   0|\n",
      "|2023-12-26 23:00_48|2023-12-26 23:00|              48|                        0|                   0|\n",
      "| 2023-12-26 23:00_1|2023-12-26 23:00|               1|                        0|                   0|\n",
      "| 2023-12-26 23:00_2|2023-12-26 23:00|               2|                        0|                   0|\n",
      "|2023-12-26 23:00_57|2023-12-26 23:00|              57|                        0|                   0|\n",
      "|2023-12-26 23:00_20|2023-12-26 23:00|              20|                        0|                   0|\n",
      "|2023-12-26 23:00_59|2023-12-26 23:00|              59|                        0|                   0|\n",
      "|2023-12-26 23:00_21|2023-12-26 23:00|              21|                        0|                   0|\n",
      "+-------------------+----------------+----------------+-------------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_merged.orderBy(col(\"hourly_timestamp\").desc()).show(10)\n",
    "\n",
    "# Mostrar el DataFrame ordenado\n",
    "# final_merged.show(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "81e48e2e-41b0-466f-87bc-057d9b45afd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "666936"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth, hour, dayofweek\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Create Year column\n",
    "final_merged = final_merged.withColumn('Year', year(final_merged['hourly_timestamp']))\n",
    "\n",
    "# Create Month column\n",
    "final_merged = final_merged.withColumn('Month', month(final_merged['hourly_timestamp']))\n",
    "\n",
    "# Create Day of Month column\n",
    "final_merged = final_merged.withColumn('DayOfMonth', dayofmonth(final_merged['hourly_timestamp']))\n",
    "\n",
    "# Create Hour column\n",
    "final_merged = final_merged.withColumn('Hour', hour(final_merged['hourly_timestamp']))\n",
    "\n",
    "# Create Day of Week column\n",
    "final_merged = final_merged.withColumn(\"DayOfWeek\", dayofweek(col(\"hourly_timestamp\")))\n",
    "\n",
    "# Crear la columna isWeekend\n",
    "final_merged = final_merged.withColumn(\"isWeekend\", when((col(\"DayOfWeek\") == 1) | (col(\"DayOfWeek\") == 7), 1).otherwise(0))\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "final_merged.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "16a8ea9e-f1ca-408d-a013-9063caa7175b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Especifica la ruta del archivo CSV en el sistema de archivos local\n",
    "csv_local_path = \"file:///\" + out_agg_archivo_csv\n",
    "\n",
    "# Guardar el DataFrame en formato CSV en el sistema de archivos local\n",
    "final_merged.write.mode(\"overwrite\").option(\"header\", \"true\").csv(csv_local_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1467ddcb-03f7-4caa-92b4-162a94809814",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5611f325-0846-42df-9cfe-06502ffe588b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91292cf8-9713-4c62-9d25-db04accf14f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f238fbe7-3119-4499-96a9-6ed141ad7650",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a6cbceb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Obtener la lista de ubicaciones (zonas)\n",
    "# location_ids = df.select(collect_set('origin_id').alias('locationID_list')).first()['locationID_list']\n",
    "\n",
    "# # Generar un DataFrame con todas las combinaciones de timestamp y ubicaciones\n",
    "# def generate_series(start, stop, interval, location_ids):\n",
    "#     start, stop = spark.createDataFrame([(start, stop)], (\"start\", \"stop\")).select(\n",
    "#         [col(c).cast(\"timestamp\").cast(\"long\") for c in (\"start\", \"stop\")]\n",
    "#     ).first()\n",
    "    \n",
    "#     timestamp_df = spark.range(start, stop, interval).select(\n",
    "#         col(\"id\").cast(\"timestamp\").alias(\"timestamp\")\n",
    "#     )\n",
    "    \n",
    "#     timestamp_df = timestamp_df.withColumn(\"hourly_timestamp\", date_format(col(\"timestamp\").cast(\"timestamp\"), \"yyyy-MM-dd HH:00\"))\n",
    "\n",
    "#     # Crear el DataFrame con todas las combinaciones de timestamp y ubicaciones\n",
    "#     data_schema = [StructField(\"timestamp\", TimestampType(), True),\n",
    "#                    StructField(\"hourly_timestamp\", StringType(), True),\n",
    "#                    StructField(\"locationID\", IntegerType(), True)]\n",
    "#     final_struct = StructType(fields=data_schema)\n",
    "    \n",
    "#     timestamp_location_df = spark.createDataFrame([], schema=final_struct)\n",
    "\n",
    "#     # Iterar sobre las ubicaciones y agregarlas al DataFrame\n",
    "#     for location_id in location_ids:\n",
    "#         dataframe = timestamp_df.withColumn(\"locationID\", lit(location_id))\n",
    "#         timestamp_location_df = timestamp_location_df.union(dataframe)\n",
    "\n",
    "#     return timestamp_location_df\n",
    "\n",
    "# # Generar el DataFrame de timestamp y ubicaciones\n",
    "# timestamp_location_df = generate_series(\"2022-09-12\", \"2022-10-08\", 60 * 60, location_ids)\n",
    "\n",
    "# # Convertir la ubicación a cadena y crear la columna única\n",
    "# timestamp_location_df = timestamp_location_df.withColumn(\"UniqueColumn_G\", concat_ws('_', timestamp_location_df.hourly_timestamp,\n",
    "#                                                            col(\"locationID\").cast(StringType())))\n",
    "# timestamp_location_df = timestamp_location_df.drop(\"locationID_string\")  # Eliminar la columna innecesaria\n",
    "\n",
    "# # Mostrar algunas filas para verificar\n",
    "# timestamp_location_df.show(5)\n",
    "\n",
    "# # Unir el DataFrame generado con tus datos originales\n",
    "# df_filled = timestamp_location_df.join(df, \"UniqueColumn_G\", \"left_outer\")\n",
    "\n",
    "# # Mostrar el DataFrame resultante\n",
    "# df_filled.show()\n",
    "\n",
    "# # Contar el número de filas\n",
    "# print(\"Total de filas en el DataFrame resultante:\", df_filled.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fd9ba024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "92f1a4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #TODOS LOS POLIGONOS\n",
    "\n",
    "# import os\n",
    "# import h3\n",
    "# import folium\n",
    "# import pandas as pd\n",
    "\n",
    "# repo = r'D:\\trufiapp\\GANS'\n",
    "# # csv_file_path = os.path.join(repo, 'route_info.csv')\n",
    "# # csv_save_file_path = os.path.join(repo, 'id_index_h3.csv')\n",
    "\n",
    "# # Cargar datos del archivo CSV en un DataFrame de Pandas\n",
    "# df = pd.read_csv(csv_file_path, encoding='latin-1')\n",
    "\n",
    "# # Cargar datos del archivo CSV con índices H3 en un DataFrame de Pandas\n",
    "# h3_id_df = pd.read_csv(csv_save_file_path)\n",
    "\n",
    "# # Combinar los DataFrames en base a los índices H3 para origen\n",
    "# df_combined_origin = pd.merge(df, h3_id_df, how='inner', left_on='origin_h3_index', right_on='h3_index')\n",
    "\n",
    "# # Crear un mapa con folium\n",
    "# m = folium.Map(location=[df_combined_origin['origin_latitude'].mean(), df_combined_origin['origin_longitude'].mean()], zoom_start=12)\n",
    "\n",
    "# # Conjunto para realizar un seguimiento de hexágonos impresos\n",
    "# printed_hexagons = set()\n",
    "\n",
    "# # Función para agregar polígono al mapa con etiqueta\n",
    "# def add_polygon_with_label(m, hexagon, label):\n",
    "#     folium.Polygon(\n",
    "#         locations=hexagon,\n",
    "#         color='blue',\n",
    "#         fill=True,\n",
    "#         fill_color='blue',\n",
    "#         fill_opacity=0.4,\n",
    "#         tooltip=f'Label: {label}'  # Añadir tooltip con el número de etiqueta\n",
    "#     ).add_to(m)\n",
    "\n",
    "# # Iterar sobre los registros\n",
    "# for idx, row in df_combined_origin.iterrows():\n",
    "#     hexagon_origin = h3.h3_to_geo_boundary(row['origin_h3_index'])\n",
    "    \n",
    "#     # Verificar si el hexágono ya ha sido impreso\n",
    "#     if row['origin_h3_index'] not in printed_hexagons:\n",
    "#         add_polygon_with_label(m, hexagon_origin, row['id'])  # Utilizar 'id' como etiqueta\n",
    "#         printed_hexagons.add(row['origin_h3_index'])  # Agregar el hexágono al conjunto\n",
    "\n",
    "# # Guardar el mapa de origenes y destinos como un archivo HTML\n",
    "# map_file_path = os.path.join(repo, 'h3_map_all.html')\n",
    "# m.save(map_file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "pyspark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
