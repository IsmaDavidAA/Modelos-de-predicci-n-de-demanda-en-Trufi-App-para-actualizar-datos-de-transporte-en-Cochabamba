# Diplomado de estadistica aplicada a la toma de desiciones

## Proyecto Trufi App: Estructura de Carpetas

### Dataset

El directorio "dataset" contiene datos esenciales para el proyecto:

- **Datos Climáticos:** Información sobre temperatura, humedad y precipitación.
- **Polígonos de Cochabamba:** Límites geográficos de Cochabamba.
- **Registros de Trufi App:** Datos brutos de registros de Trufi App.
- **Resultados de Predicción:** Datos de resultados de los modelos de predicción.
- **Rutas de Cochabamba:** Detalles de las rutas registradas en Cochabamba.
  Download the data here:
  https://drive.google.com/drive/u/1/folders/1bW-L1yJLdUJqvXBZ3QU3W0mWOn7QUP-Z (request access)

### ETL (Extracción, Transformación, Carga)

La sección "ETL" abarca los procesos de Extracción, Transformación y Carga gestionados a través de Pentaho. Los componentes clave incluyen:

- **Carpeta de Jobs:** Contiene trabajos Pentaho como "Get Logs" y "Trufi ETL."
- **Carpeta de Scripts:** Scripts en Python ejecutados por los procesos ETL.
- **Carpeta de Transformaciones:** Incluye transformaciones para extraer logs, notas y rutas.

### Jupyter Notebooks

El directorio "Jupyter Notebooks" organiza cuadernos para varias etapas del proyecto:

- **Extracción:** Cuaderno para la extracción de datos.
- **Transformación:** Cuaderno para la transformación de datos.
- **Análisis de Datos (EDA):** Cuaderno para el análisis exploratorio de datos.
- **Implementación de Deep Learning:** Cuaderno para la implementación de modelos de deep learning.
- **Implementación de Machine Learning:** Cuaderno para la implementación de modelos de machine learning.
- **Predicción:** Cuaderno para el proceso de predicción.
- **Exportar Predicción:** Cuaderno para exportar los resultados de la predicción.

### Predicción

En el directorio "Predicción":

- **Modelos:** Almacena modelos guardados para referencia.
- **Datos de Predicción:** Contiene datos de resultados de predicción en formato CSV.

Siéntete libre de explorar cada sección para obtener información sobre el conjunto de datos, los procesos ETL y la implementación de modelos de machine learning y deep learning. Para instrucciones detalladas, consulta la documentación en las carpetas respectivas.
